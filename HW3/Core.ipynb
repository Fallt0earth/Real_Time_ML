{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa35c075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "import torch\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "from torchvision import datasets\n",
    "data_path = \"./data/\"\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True)\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e2abd53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32, 10000])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "tensor_cifar10 = datasets.CIFAR10(data_path, train=True, download=False, transform=transforms.ToTensor())\n",
    "imgs = torch.stack([img_t for img_t, _ in tensor_cifar10], dim=3)\n",
    "imgs.shape\n",
    "tensor_cifar10_val = datasets.CIFAR10(data_path, train=False, download=False, transform=transforms.ToTensor())\n",
    "imgs_val = torch.stack([img_t for img_t, _ in tensor_cifar10_val], dim=3)\n",
    "imgs_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1754447a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.Normalize((0.4914, 0.4822, 0.4465),  (0.2470, 0.2435, 0.2616))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "81189c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "transformed_cifar10 = datasets.CIFAR10(data_path, train=True, download=False, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465),  (0.2470, 0.2435, 0.2616))]))\n",
    "img_t, _  = transformed_cifar10[99]\n",
    "plt.imshow(img_t.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9692384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms.Normalize((0.4942, 0.4851, 0.4504),  (0.2467, 0.2429, 0.2616))\n",
    "val_transformed_cifar10 = datasets.CIFAR10(data_path, train=False, download=False, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465),  (0.2470, 0.2435, 0.2616))]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8abb4a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=5):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c8a4a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda:0')\n",
    "train_loader = torch.utils.data.DataLoader(transformed_cifar10, batch_size=64, shuffle=True)\n",
    "\n",
    "model = CNN()\n",
    "model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5339d0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def training(model, optimizer, loss_fn, n_epochs, device, train_loader):\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            batch_size = imgs.shape[0]\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn((outputs), labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1efbc42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.498447\n",
      "Epoch: 1, Loss: 1.497896\n",
      "Epoch: 2, Loss: 1.313029\n",
      "Epoch: 3, Loss: 0.725449\n",
      "Epoch: 4, Loss: 1.140860\n",
      "Epoch: 5, Loss: 0.979368\n",
      "Epoch: 6, Loss: 0.955193\n",
      "Epoch: 7, Loss: 0.922603\n",
      "Epoch: 8, Loss: 0.877520\n",
      "Epoch: 9, Loss: 1.048637\n",
      "Epoch: 10, Loss: 0.969260\n",
      "Epoch: 11, Loss: 0.982045\n",
      "Epoch: 12, Loss: 0.776672\n",
      "Epoch: 13, Loss: 0.345156\n",
      "Epoch: 14, Loss: 0.983632\n",
      "Epoch: 15, Loss: 0.762592\n",
      "Epoch: 16, Loss: 0.937057\n",
      "Epoch: 17, Loss: 0.873066\n",
      "Epoch: 18, Loss: 0.701364\n",
      "Epoch: 19, Loss: 0.904475\n",
      "Epoch: 20, Loss: 0.554761\n",
      "Epoch: 21, Loss: 1.172717\n",
      "Epoch: 22, Loss: 0.972105\n",
      "Epoch: 23, Loss: 0.560831\n",
      "Epoch: 24, Loss: 0.362465\n",
      "Epoch: 25, Loss: 0.488543\n",
      "Epoch: 26, Loss: 0.514368\n",
      "Epoch: 27, Loss: 0.417470\n",
      "Epoch: 28, Loss: 0.634012\n",
      "Epoch: 29, Loss: 0.876366\n",
      "Epoch: 30, Loss: 0.836783\n",
      "Epoch: 31, Loss: 0.840493\n",
      "Epoch: 32, Loss: 0.869559\n",
      "Epoch: 33, Loss: 0.839005\n",
      "Epoch: 34, Loss: 1.090934\n",
      "Epoch: 35, Loss: 0.555672\n",
      "Epoch: 36, Loss: 0.724887\n",
      "Epoch: 37, Loss: 0.855388\n",
      "Epoch: 38, Loss: 0.477666\n",
      "Epoch: 39, Loss: 0.570756\n",
      "Epoch: 40, Loss: 0.477735\n",
      "Epoch: 41, Loss: 0.197177\n",
      "Epoch: 42, Loss: 0.531599\n",
      "Epoch: 43, Loss: 0.571237\n",
      "Epoch: 44, Loss: 0.346088\n",
      "Epoch: 45, Loss: 0.477955\n",
      "Epoch: 46, Loss: 0.348622\n",
      "Epoch: 47, Loss: 0.376266\n",
      "Epoch: 48, Loss: 0.588658\n",
      "Epoch: 49, Loss: 0.534189\n",
      "Epoch: 50, Loss: 0.496581\n",
      "Epoch: 51, Loss: 0.524668\n",
      "Epoch: 52, Loss: 0.512729\n",
      "Epoch: 53, Loss: 0.410188\n",
      "Epoch: 54, Loss: 0.632845\n",
      "Epoch: 55, Loss: 0.855601\n",
      "Epoch: 56, Loss: 0.321324\n",
      "Epoch: 57, Loss: 0.616294\n",
      "Epoch: 58, Loss: 0.762693\n",
      "Epoch: 59, Loss: 0.605400\n",
      "Epoch: 60, Loss: 0.404862\n",
      "Epoch: 61, Loss: 0.392822\n",
      "Epoch: 62, Loss: 0.694334\n",
      "Epoch: 63, Loss: 0.506469\n",
      "Epoch: 64, Loss: 0.725394\n",
      "Epoch: 65, Loss: 0.351106\n",
      "Epoch: 66, Loss: 0.326640\n",
      "Epoch: 67, Loss: 0.358067\n",
      "Epoch: 68, Loss: 0.347434\n",
      "Epoch: 69, Loss: 0.277963\n",
      "Epoch: 70, Loss: 0.242718\n",
      "Epoch: 71, Loss: 0.523904\n",
      "Epoch: 72, Loss: 0.572263\n",
      "Epoch: 73, Loss: 0.399414\n",
      "Epoch: 74, Loss: 0.857042\n",
      "Epoch: 75, Loss: 0.556908\n",
      "Epoch: 76, Loss: 0.580345\n",
      "Epoch: 77, Loss: 0.475995\n",
      "Epoch: 78, Loss: 0.675956\n",
      "Epoch: 79, Loss: 0.484926\n",
      "Epoch: 80, Loss: 0.627784\n",
      "Epoch: 81, Loss: 0.701546\n",
      "Epoch: 82, Loss: 0.192024\n",
      "Epoch: 83, Loss: 0.220258\n",
      "Epoch: 84, Loss: 0.600897\n",
      "Epoch: 85, Loss: 0.427027\n",
      "Epoch: 86, Loss: 0.357047\n",
      "Epoch: 87, Loss: 0.270161\n",
      "Epoch: 88, Loss: 0.182802\n",
      "Epoch: 89, Loss: 0.320588\n",
      "Epoch: 90, Loss: 0.169319\n",
      "Epoch: 91, Loss: 0.604976\n",
      "Epoch: 92, Loss: 0.244069\n",
      "Epoch: 93, Loss: 0.278588\n",
      "Epoch: 94, Loss: 0.361861\n",
      "Epoch: 95, Loss: 0.168244\n",
      "Epoch: 96, Loss: 0.515295\n",
      "Epoch: 97, Loss: 0.380831\n",
      "Epoch: 98, Loss: 0.101507\n",
      "Epoch: 99, Loss: 0.417905\n",
      "Epoch: 100, Loss: 0.584851\n",
      "Epoch: 101, Loss: 0.358312\n",
      "Epoch: 102, Loss: 0.163078\n",
      "Epoch: 103, Loss: 0.154197\n",
      "Epoch: 104, Loss: 0.166170\n",
      "Epoch: 105, Loss: 0.150583\n",
      "Epoch: 106, Loss: 0.218236\n",
      "Epoch: 107, Loss: 0.378485\n",
      "Epoch: 108, Loss: 0.255147\n",
      "Epoch: 109, Loss: 0.297278\n",
      "Epoch: 110, Loss: 0.192649\n",
      "Epoch: 111, Loss: 0.205934\n",
      "Epoch: 112, Loss: 0.101475\n",
      "Epoch: 113, Loss: 0.155968\n",
      "Epoch: 114, Loss: 0.240980\n",
      "Epoch: 115, Loss: 0.129733\n",
      "Epoch: 116, Loss: 0.067875\n",
      "Epoch: 117, Loss: 0.116800\n",
      "Epoch: 118, Loss: 0.168921\n",
      "Epoch: 119, Loss: 0.152258\n",
      "Epoch: 120, Loss: 0.223537\n",
      "Epoch: 121, Loss: 0.289648\n",
      "Epoch: 122, Loss: 0.386241\n",
      "Epoch: 123, Loss: 0.199642\n",
      "Epoch: 124, Loss: 0.389101\n",
      "Epoch: 125, Loss: 0.112741\n",
      "Epoch: 126, Loss: 0.455202\n",
      "Epoch: 127, Loss: 0.115323\n",
      "Epoch: 128, Loss: 0.124189\n",
      "Epoch: 129, Loss: 0.096847\n",
      "Epoch: 130, Loss: 0.146255\n",
      "Epoch: 131, Loss: 0.137659\n",
      "Epoch: 132, Loss: 0.323041\n",
      "Epoch: 133, Loss: 0.142082\n",
      "Epoch: 134, Loss: 0.188160\n",
      "Epoch: 135, Loss: 0.113059\n",
      "Epoch: 136, Loss: 0.033434\n",
      "Epoch: 137, Loss: 0.097937\n",
      "Epoch: 138, Loss: 0.085505\n",
      "Epoch: 139, Loss: 0.216704\n",
      "Epoch: 140, Loss: 0.075677\n",
      "Epoch: 141, Loss: 0.151857\n",
      "Epoch: 142, Loss: 0.299328\n",
      "Epoch: 143, Loss: 0.122950\n",
      "Epoch: 144, Loss: 0.108964\n",
      "Epoch: 145, Loss: 0.172743\n",
      "Epoch: 146, Loss: 0.218228\n",
      "Epoch: 147, Loss: 0.084273\n",
      "Epoch: 148, Loss: 0.156723\n",
      "Epoch: 149, Loss: 0.103930\n",
      "Epoch: 150, Loss: 0.064997\n",
      "Epoch: 151, Loss: 0.123085\n",
      "Epoch: 152, Loss: 0.084942\n",
      "Epoch: 153, Loss: 0.187394\n",
      "Epoch: 154, Loss: 0.091330\n",
      "Epoch: 155, Loss: 0.109375\n",
      "Epoch: 156, Loss: 0.140404\n",
      "Epoch: 157, Loss: 0.149521\n",
      "Epoch: 158, Loss: 0.124805\n",
      "Epoch: 159, Loss: 0.106408\n",
      "Epoch: 160, Loss: 0.039498\n",
      "Epoch: 161, Loss: 0.106182\n",
      "Epoch: 162, Loss: 0.039681\n",
      "Epoch: 163, Loss: 0.095973\n",
      "Epoch: 164, Loss: 0.086228\n",
      "Epoch: 165, Loss: 0.085590\n",
      "Epoch: 166, Loss: 0.054242\n",
      "Epoch: 167, Loss: 0.243868\n",
      "Epoch: 168, Loss: 0.038838\n",
      "Epoch: 169, Loss: 0.039462\n",
      "Epoch: 170, Loss: 0.139226\n",
      "Epoch: 171, Loss: 0.110857\n",
      "Epoch: 172, Loss: 0.036935\n",
      "Epoch: 173, Loss: 0.106023\n",
      "Epoch: 174, Loss: 0.040010\n",
      "Epoch: 175, Loss: 0.016204\n",
      "Epoch: 176, Loss: 0.047791\n",
      "Epoch: 177, Loss: 0.083080\n",
      "Epoch: 178, Loss: 0.095875\n",
      "Epoch: 179, Loss: 0.056258\n",
      "Epoch: 180, Loss: 0.035833\n",
      "Epoch: 181, Loss: 0.028847\n",
      "Epoch: 182, Loss: 0.040698\n",
      "Epoch: 183, Loss: 0.054574\n",
      "Epoch: 184, Loss: 0.052165\n",
      "Epoch: 185, Loss: 0.035904\n",
      "Epoch: 186, Loss: 0.018172\n",
      "Epoch: 187, Loss: 0.012073\n",
      "Epoch: 188, Loss: 0.036133\n",
      "Epoch: 189, Loss: 0.026720\n",
      "Epoch: 190, Loss: 0.056869\n",
      "Epoch: 191, Loss: 0.024018\n",
      "Epoch: 192, Loss: 0.037674\n",
      "Epoch: 193, Loss: 0.036731\n",
      "Epoch: 194, Loss: 0.097569\n",
      "Epoch: 195, Loss: 0.060853\n",
      "Epoch: 196, Loss: 0.019426\n",
      "Epoch: 197, Loss: 0.031833\n",
      "Epoch: 198, Loss: 0.047987\n",
      "Epoch: 199, Loss: 0.036798\n",
      "Epoch: 200, Loss: 0.046242\n",
      "Epoch: 201, Loss: 0.020834\n",
      "Epoch: 202, Loss: 0.025545\n",
      "Epoch: 203, Loss: 0.067831\n",
      "Epoch: 204, Loss: 0.010615\n",
      "Epoch: 205, Loss: 0.034919\n",
      "Epoch: 206, Loss: 0.180730\n",
      "Epoch: 207, Loss: 0.021324\n",
      "Epoch: 208, Loss: 0.028950\n",
      "Epoch: 209, Loss: 0.021941\n",
      "Epoch: 210, Loss: 0.025035\n",
      "Epoch: 211, Loss: 0.046879\n",
      "Epoch: 212, Loss: 0.011836\n",
      "Epoch: 213, Loss: 0.020497\n",
      "Epoch: 214, Loss: 0.021238\n",
      "Epoch: 215, Loss: 0.017562\n",
      "Epoch: 216, Loss: 0.015915\n",
      "Epoch: 217, Loss: 0.035125\n",
      "Epoch: 218, Loss: 0.030752\n",
      "Epoch: 219, Loss: 0.037759\n",
      "Epoch: 220, Loss: 0.012466\n",
      "Epoch: 221, Loss: 0.026364\n",
      "Epoch: 222, Loss: 0.021804\n",
      "Epoch: 223, Loss: 0.015536\n",
      "Epoch: 224, Loss: 0.010265\n",
      "Epoch: 225, Loss: 0.019381\n",
      "Epoch: 226, Loss: 0.013898\n",
      "Epoch: 227, Loss: 0.013814\n",
      "Epoch: 228, Loss: 0.016014\n",
      "Epoch: 229, Loss: 0.016466\n",
      "Epoch: 230, Loss: 0.007997\n",
      "Epoch: 231, Loss: 0.009654\n",
      "Epoch: 232, Loss: 0.010657\n",
      "Epoch: 233, Loss: 0.017986\n",
      "Epoch: 234, Loss: 0.008797\n",
      "Epoch: 235, Loss: 0.007140\n",
      "Epoch: 236, Loss: 0.006865\n",
      "Epoch: 237, Loss: 0.016767\n",
      "Epoch: 238, Loss: 0.009168\n",
      "Epoch: 239, Loss: 0.017544\n",
      "Epoch: 240, Loss: 0.048089\n",
      "Epoch: 241, Loss: 0.016886\n",
      "Epoch: 242, Loss: 0.013881\n",
      "Epoch: 243, Loss: 0.028045\n",
      "Epoch: 244, Loss: 0.012124\n",
      "Epoch: 245, Loss: 0.003064\n",
      "Epoch: 246, Loss: 0.025037\n",
      "Epoch: 247, Loss: 0.003216\n",
      "Epoch: 248, Loss: 0.007882\n",
      "Epoch: 249, Loss: 0.005541\n",
      "Epoch: 250, Loss: 0.001212\n",
      "Epoch: 251, Loss: 0.005447\n",
      "Epoch: 252, Loss: 0.007220\n",
      "Epoch: 253, Loss: 0.008085\n",
      "Epoch: 254, Loss: 0.005604\n",
      "Epoch: 255, Loss: 0.006485\n",
      "Epoch: 256, Loss: 0.002781\n",
      "Epoch: 257, Loss: 0.002156\n",
      "Epoch: 258, Loss: 0.011245\n",
      "Epoch: 259, Loss: 0.003058\n",
      "Epoch: 260, Loss: 0.004969\n",
      "Epoch: 261, Loss: 0.005466\n",
      "Epoch: 262, Loss: 0.006512\n",
      "Epoch: 263, Loss: 0.001763\n",
      "Epoch: 264, Loss: 0.004633\n",
      "Epoch: 265, Loss: 0.001871\n",
      "Epoch: 266, Loss: 0.008104\n",
      "Epoch: 267, Loss: 0.014022\n",
      "Epoch: 268, Loss: 0.011453\n",
      "Epoch: 269, Loss: 0.001730\n",
      "Epoch: 270, Loss: 0.005452\n",
      "Epoch: 271, Loss: 0.003399\n",
      "Epoch: 272, Loss: 0.002999\n",
      "Epoch: 273, Loss: 0.001531\n",
      "Epoch: 274, Loss: 0.013616\n",
      "Epoch: 275, Loss: 0.004004\n",
      "Epoch: 276, Loss: 0.015762\n",
      "Epoch: 277, Loss: 0.002352\n",
      "Epoch: 278, Loss: 0.002196\n",
      "Epoch: 279, Loss: 0.010473\n",
      "Epoch: 280, Loss: 0.007576\n",
      "Epoch: 281, Loss: 0.002678\n",
      "Epoch: 282, Loss: 0.003726\n",
      "Epoch: 283, Loss: 0.001975\n",
      "Epoch: 284, Loss: 0.006700\n",
      "Epoch: 285, Loss: 0.002702\n",
      "Epoch: 286, Loss: 0.004033\n",
      "Epoch: 287, Loss: 0.004906\n",
      "Epoch: 288, Loss: 0.005658\n",
      "Epoch: 289, Loss: 0.002335\n",
      "Epoch: 290, Loss: 0.002274\n",
      "Epoch: 291, Loss: 0.002801\n",
      "Epoch: 292, Loss: 0.002769\n",
      "Epoch: 293, Loss: 0.002543\n",
      "Epoch: 294, Loss: 0.005466\n",
      "Epoch: 295, Loss: 0.004295\n",
      "Epoch: 296, Loss: 0.004386\n",
      "Epoch: 297, Loss: 0.003075\n",
      "Epoch: 298, Loss: 0.000987\n",
      "Epoch: 299, Loss: 0.002263\n",
      "2617.700516939163\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 300\n",
    "training(model, optimizer, loss_function, n_epochs, device, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8ec98b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational complexity:       0.0 GMac\n",
      "Number of parameters:           76.04 k \n"
     ]
    }
   ],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "\n",
    "macs, params = get_model_complexity_info(model, ( 3, 32,32), as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f2b92375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  0.6443\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(val_transformed_cifar10, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size=imgs.shape[0]\n",
    "        outputs = model(imgs)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "#         print(predicted)\n",
    "#         print(\"\\n\")\n",
    "#         print(labels)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted==labels).sum())\n",
    "    print(\"Accuracy \", correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a9c535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1b\n",
    "import torch.nn.functional as F\n",
    "class CNN2(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=5):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, 16, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, n_chans1, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv3(out)), 2)\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3109c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda:0')\n",
    "train_loader = torch.utils.data.DataLoader(transformed_cifar10, batch_size=64, shuffle=True)\n",
    "\n",
    "model = CNN2()\n",
    "model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "89e870d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def training(model, optimizer, loss_fn, n_epochs, device, train_loader):\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            batch_size = imgs.shape[0]\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn((outputs), labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fcc8beeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.896323\n",
      "Epoch: 1, Loss: 1.648372\n",
      "Epoch: 2, Loss: 1.083320\n",
      "Epoch: 3, Loss: 1.322986\n",
      "Epoch: 4, Loss: 1.201101\n",
      "Epoch: 5, Loss: 1.132529\n",
      "Epoch: 6, Loss: 1.267703\n",
      "Epoch: 7, Loss: 1.157219\n",
      "Epoch: 8, Loss: 1.527774\n",
      "Epoch: 9, Loss: 0.892078\n",
      "Epoch: 10, Loss: 0.720922\n",
      "Epoch: 11, Loss: 1.120539\n",
      "Epoch: 12, Loss: 0.693970\n",
      "Epoch: 13, Loss: 1.180377\n",
      "Epoch: 14, Loss: 1.237997\n",
      "Epoch: 15, Loss: 0.881170\n",
      "Epoch: 16, Loss: 0.662154\n",
      "Epoch: 17, Loss: 1.143472\n",
      "Epoch: 18, Loss: 0.830828\n",
      "Epoch: 19, Loss: 0.953425\n",
      "Epoch: 20, Loss: 1.368803\n",
      "Epoch: 21, Loss: 1.404832\n",
      "Epoch: 22, Loss: 0.600543\n",
      "Epoch: 23, Loss: 0.858952\n",
      "Epoch: 24, Loss: 0.946415\n",
      "Epoch: 25, Loss: 0.583561\n",
      "Epoch: 26, Loss: 0.418478\n",
      "Epoch: 27, Loss: 0.705524\n",
      "Epoch: 28, Loss: 0.718551\n",
      "Epoch: 29, Loss: 0.548431\n",
      "Epoch: 30, Loss: 0.621175\n",
      "Epoch: 31, Loss: 0.785009\n",
      "Epoch: 32, Loss: 0.765577\n",
      "Epoch: 33, Loss: 0.861708\n",
      "Epoch: 34, Loss: 0.878913\n",
      "Epoch: 35, Loss: 0.491723\n",
      "Epoch: 36, Loss: 0.741196\n",
      "Epoch: 37, Loss: 0.602203\n",
      "Epoch: 38, Loss: 0.510127\n",
      "Epoch: 39, Loss: 0.564570\n",
      "Epoch: 40, Loss: 0.663044\n",
      "Epoch: 41, Loss: 0.779188\n",
      "Epoch: 42, Loss: 0.820207\n",
      "Epoch: 43, Loss: 0.959233\n",
      "Epoch: 44, Loss: 1.153374\n",
      "Epoch: 45, Loss: 0.873901\n",
      "Epoch: 46, Loss: 0.847194\n",
      "Epoch: 47, Loss: 0.733507\n",
      "Epoch: 48, Loss: 0.779626\n",
      "Epoch: 49, Loss: 0.694241\n",
      "Epoch: 50, Loss: 0.260936\n",
      "Epoch: 51, Loss: 0.729337\n",
      "Epoch: 52, Loss: 0.960977\n",
      "Epoch: 53, Loss: 1.128400\n",
      "Epoch: 54, Loss: 0.529357\n",
      "Epoch: 55, Loss: 1.252581\n",
      "Epoch: 56, Loss: 0.933282\n",
      "Epoch: 57, Loss: 0.348256\n",
      "Epoch: 58, Loss: 0.422800\n",
      "Epoch: 59, Loss: 0.563232\n",
      "Epoch: 60, Loss: 0.424046\n",
      "Epoch: 61, Loss: 0.856917\n",
      "Epoch: 62, Loss: 0.405780\n",
      "Epoch: 63, Loss: 0.423738\n",
      "Epoch: 64, Loss: 0.815334\n",
      "Epoch: 65, Loss: 0.363746\n",
      "Epoch: 66, Loss: 0.484086\n",
      "Epoch: 67, Loss: 0.666310\n",
      "Epoch: 68, Loss: 0.289109\n",
      "Epoch: 69, Loss: 0.574485\n",
      "Epoch: 70, Loss: 0.468145\n",
      "Epoch: 71, Loss: 0.252282\n",
      "Epoch: 72, Loss: 0.709677\n",
      "Epoch: 73, Loss: 0.909277\n",
      "Epoch: 74, Loss: 0.386946\n",
      "Epoch: 75, Loss: 0.479924\n",
      "Epoch: 76, Loss: 0.933227\n",
      "Epoch: 77, Loss: 0.691403\n",
      "Epoch: 78, Loss: 0.275495\n",
      "Epoch: 79, Loss: 0.697377\n",
      "Epoch: 80, Loss: 0.633488\n",
      "Epoch: 81, Loss: 0.812485\n",
      "Epoch: 82, Loss: 0.430187\n",
      "Epoch: 83, Loss: 0.645006\n",
      "Epoch: 84, Loss: 1.022087\n",
      "Epoch: 85, Loss: 0.619434\n",
      "Epoch: 86, Loss: 0.336618\n",
      "Epoch: 87, Loss: 0.451006\n",
      "Epoch: 88, Loss: 0.533008\n",
      "Epoch: 89, Loss: 0.709450\n",
      "Epoch: 90, Loss: 0.401480\n",
      "Epoch: 91, Loss: 0.607547\n",
      "Epoch: 92, Loss: 0.498062\n",
      "Epoch: 93, Loss: 0.641506\n",
      "Epoch: 94, Loss: 0.450096\n",
      "Epoch: 95, Loss: 0.859753\n",
      "Epoch: 96, Loss: 0.669792\n",
      "Epoch: 97, Loss: 0.406585\n",
      "Epoch: 98, Loss: 0.705167\n",
      "Epoch: 99, Loss: 0.330190\n",
      "Epoch: 100, Loss: 0.901043\n",
      "Epoch: 101, Loss: 0.464197\n",
      "Epoch: 102, Loss: 0.640942\n",
      "Epoch: 103, Loss: 0.424082\n",
      "Epoch: 104, Loss: 0.682475\n",
      "Epoch: 105, Loss: 0.550172\n",
      "Epoch: 106, Loss: 0.548426\n",
      "Epoch: 107, Loss: 0.263066\n",
      "Epoch: 108, Loss: 0.268184\n",
      "Epoch: 109, Loss: 0.438816\n",
      "Epoch: 110, Loss: 0.346763\n",
      "Epoch: 111, Loss: 0.345796\n",
      "Epoch: 112, Loss: 0.416866\n",
      "Epoch: 113, Loss: 0.662212\n",
      "Epoch: 114, Loss: 0.549651\n",
      "Epoch: 115, Loss: 0.440759\n",
      "Epoch: 116, Loss: 0.792771\n",
      "Epoch: 117, Loss: 0.307351\n",
      "Epoch: 118, Loss: 0.638000\n",
      "Epoch: 119, Loss: 0.490885\n",
      "Epoch: 120, Loss: 0.375789\n",
      "Epoch: 121, Loss: 0.548913\n",
      "Epoch: 122, Loss: 0.714649\n",
      "Epoch: 123, Loss: 0.360686\n",
      "Epoch: 124, Loss: 0.433685\n",
      "Epoch: 125, Loss: 0.368262\n",
      "Epoch: 126, Loss: 0.721726\n",
      "Epoch: 127, Loss: 0.535415\n",
      "Epoch: 128, Loss: 0.627891\n",
      "Epoch: 129, Loss: 0.270918\n",
      "Epoch: 130, Loss: 0.550155\n",
      "Epoch: 131, Loss: 0.671316\n",
      "Epoch: 132, Loss: 0.600900\n",
      "Epoch: 133, Loss: 0.427695\n",
      "Epoch: 134, Loss: 0.190060\n",
      "Epoch: 135, Loss: 0.468132\n",
      "Epoch: 136, Loss: 0.363780\n",
      "Epoch: 137, Loss: 0.755758\n",
      "Epoch: 138, Loss: 0.239808\n",
      "Epoch: 139, Loss: 0.167057\n",
      "Epoch: 140, Loss: 0.425273\n",
      "Epoch: 141, Loss: 0.223723\n",
      "Epoch: 142, Loss: 0.458519\n",
      "Epoch: 143, Loss: 0.376198\n",
      "Epoch: 144, Loss: 0.405882\n",
      "Epoch: 145, Loss: 0.495435\n",
      "Epoch: 146, Loss: 0.384176\n",
      "Epoch: 147, Loss: 0.464494\n",
      "Epoch: 148, Loss: 0.250836\n",
      "Epoch: 149, Loss: 0.417796\n",
      "Epoch: 150, Loss: 0.526215\n",
      "Epoch: 151, Loss: 0.267406\n",
      "Epoch: 152, Loss: 0.148104\n",
      "Epoch: 153, Loss: 0.211547\n",
      "Epoch: 154, Loss: 0.406436\n",
      "Epoch: 155, Loss: 0.495581\n",
      "Epoch: 156, Loss: 0.177228\n",
      "Epoch: 157, Loss: 0.251501\n",
      "Epoch: 158, Loss: 0.266415\n",
      "Epoch: 159, Loss: 0.231716\n",
      "Epoch: 160, Loss: 0.729060\n",
      "Epoch: 161, Loss: 0.401378\n",
      "Epoch: 162, Loss: 0.234087\n",
      "Epoch: 163, Loss: 0.551553\n",
      "Epoch: 164, Loss: 0.478218\n",
      "Epoch: 165, Loss: 0.371285\n",
      "Epoch: 166, Loss: 0.105495\n",
      "Epoch: 167, Loss: 0.349293\n",
      "Epoch: 168, Loss: 0.516784\n",
      "Epoch: 169, Loss: 0.424125\n",
      "Epoch: 170, Loss: 0.270033\n",
      "Epoch: 171, Loss: 0.198812\n",
      "Epoch: 172, Loss: 0.236980\n",
      "Epoch: 173, Loss: 0.407601\n",
      "Epoch: 174, Loss: 0.317935\n",
      "Epoch: 175, Loss: 0.923415\n",
      "Epoch: 176, Loss: 1.208022\n",
      "Epoch: 177, Loss: 0.412102\n",
      "Epoch: 178, Loss: 0.307048\n",
      "Epoch: 179, Loss: 0.221508\n",
      "Epoch: 180, Loss: 0.191000\n",
      "Epoch: 181, Loss: 0.635185\n",
      "Epoch: 182, Loss: 0.353264\n",
      "Epoch: 183, Loss: 0.386095\n",
      "Epoch: 184, Loss: 0.789814\n",
      "Epoch: 185, Loss: 0.616831\n",
      "Epoch: 186, Loss: 0.338618\n",
      "Epoch: 187, Loss: 0.424749\n",
      "Epoch: 188, Loss: 0.352902\n",
      "Epoch: 189, Loss: 0.327687\n",
      "Epoch: 190, Loss: 0.128037\n",
      "Epoch: 191, Loss: 0.136699\n",
      "Epoch: 192, Loss: 0.275025\n",
      "Epoch: 193, Loss: 0.267811\n",
      "Epoch: 194, Loss: 0.188329\n",
      "Epoch: 195, Loss: 0.245726\n",
      "Epoch: 196, Loss: 0.379887\n",
      "Epoch: 197, Loss: 0.360750\n",
      "Epoch: 198, Loss: 0.563856\n",
      "Epoch: 199, Loss: 0.469363\n",
      "Epoch: 200, Loss: 0.541940\n",
      "Epoch: 201, Loss: 0.302168\n",
      "Epoch: 202, Loss: 0.258219\n",
      "Epoch: 203, Loss: 0.419230\n",
      "Epoch: 204, Loss: 0.348259\n",
      "Epoch: 205, Loss: 0.491737\n",
      "Epoch: 206, Loss: 0.589031\n",
      "Epoch: 207, Loss: 0.350189\n",
      "Epoch: 208, Loss: 0.156740\n",
      "Epoch: 209, Loss: 0.306332\n",
      "Epoch: 210, Loss: 0.126633\n",
      "Epoch: 211, Loss: 0.236091\n",
      "Epoch: 212, Loss: 0.550422\n",
      "Epoch: 213, Loss: 0.662598\n",
      "Epoch: 214, Loss: 0.488895\n",
      "Epoch: 215, Loss: 0.399305\n",
      "Epoch: 216, Loss: 0.159977\n",
      "Epoch: 217, Loss: 0.372328\n",
      "Epoch: 218, Loss: 0.192008\n",
      "Epoch: 219, Loss: 0.123877\n",
      "Epoch: 220, Loss: 0.390310\n",
      "Epoch: 221, Loss: 0.397420\n",
      "Epoch: 222, Loss: 0.759552\n",
      "Epoch: 223, Loss: 0.293446\n",
      "Epoch: 224, Loss: 0.221819\n",
      "Epoch: 225, Loss: 0.352326\n",
      "Epoch: 226, Loss: 0.176500\n",
      "Epoch: 227, Loss: 0.688282\n",
      "Epoch: 228, Loss: 0.507022\n",
      "Epoch: 229, Loss: 0.734138\n",
      "Epoch: 230, Loss: 0.217383\n",
      "Epoch: 231, Loss: 0.125206\n",
      "Epoch: 232, Loss: 0.372049\n",
      "Epoch: 233, Loss: 0.564945\n",
      "Epoch: 234, Loss: 0.303220\n",
      "Epoch: 235, Loss: 0.191691\n",
      "Epoch: 236, Loss: 0.656408\n",
      "Epoch: 237, Loss: 0.297436\n",
      "Epoch: 238, Loss: 0.143998\n",
      "Epoch: 239, Loss: 0.242424\n",
      "Epoch: 240, Loss: 0.367371\n",
      "Epoch: 241, Loss: 0.249586\n",
      "Epoch: 242, Loss: 0.334356\n",
      "Epoch: 243, Loss: 0.179882\n",
      "Epoch: 244, Loss: 0.172921\n",
      "Epoch: 245, Loss: 0.163452\n",
      "Epoch: 246, Loss: 0.383987\n",
      "Epoch: 247, Loss: 0.441124\n",
      "Epoch: 248, Loss: 0.672070\n",
      "Epoch: 249, Loss: 0.188298\n",
      "Epoch: 250, Loss: 0.351831\n",
      "Epoch: 251, Loss: 0.100963\n",
      "Epoch: 252, Loss: 0.217324\n",
      "Epoch: 253, Loss: 0.330780\n",
      "Epoch: 254, Loss: 0.350682\n",
      "Epoch: 255, Loss: 0.420344\n",
      "Epoch: 256, Loss: 0.273676\n",
      "Epoch: 257, Loss: 0.162933\n",
      "Epoch: 258, Loss: 0.640054\n",
      "Epoch: 259, Loss: 0.284244\n",
      "Epoch: 260, Loss: 0.328823\n",
      "Epoch: 261, Loss: 0.357020\n",
      "Epoch: 262, Loss: 0.296699\n",
      "Epoch: 263, Loss: 0.395608\n",
      "Epoch: 264, Loss: 0.365300\n",
      "Epoch: 265, Loss: 0.418496\n",
      "Epoch: 266, Loss: 0.095528\n",
      "Epoch: 267, Loss: 0.730917\n",
      "Epoch: 268, Loss: 0.223145\n",
      "Epoch: 269, Loss: 0.129105\n",
      "Epoch: 270, Loss: 0.529553\n",
      "Epoch: 271, Loss: 0.438484\n",
      "Epoch: 272, Loss: 0.220422\n",
      "Epoch: 273, Loss: 0.406033\n",
      "Epoch: 274, Loss: 0.497470\n",
      "Epoch: 275, Loss: 0.611705\n",
      "Epoch: 276, Loss: 0.405171\n",
      "Epoch: 277, Loss: 0.298941\n",
      "Epoch: 278, Loss: 0.060136\n",
      "Epoch: 279, Loss: 0.278914\n",
      "Epoch: 280, Loss: 0.272528\n",
      "Epoch: 281, Loss: 0.319582\n",
      "Epoch: 282, Loss: 0.272578\n",
      "Epoch: 283, Loss: 0.196376\n",
      "Epoch: 284, Loss: 0.249674\n",
      "Epoch: 285, Loss: 0.664153\n",
      "Epoch: 286, Loss: 0.230481\n",
      "Epoch: 287, Loss: 0.176848\n",
      "Epoch: 288, Loss: 0.213954\n",
      "Epoch: 289, Loss: 0.899344\n",
      "Epoch: 290, Loss: 0.942388\n",
      "Epoch: 291, Loss: 0.361399\n",
      "Epoch: 292, Loss: 0.130273\n",
      "Epoch: 293, Loss: 0.407304\n",
      "Epoch: 294, Loss: 0.219181\n",
      "Epoch: 295, Loss: 0.260964\n",
      "Epoch: 296, Loss: 0.206191\n",
      "Epoch: 297, Loss: 0.304515\n",
      "Epoch: 298, Loss: 0.171799\n",
      "Epoch: 299, Loss: 0.435424\n",
      "2682.2555429935455\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 300\n",
    "training(model, optimizer, loss_function, n_epochs, device, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "defad2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational complexity:       0.0 GMac\n",
      "Number of parameters:           26.91 k \n"
     ]
    }
   ],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "\n",
    "macs, params = get_model_complexity_info(model, ( 3, 32,32), as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "52f2b66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  0.6989\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(val_transformed_cifar10, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size=imgs.shape[0]\n",
    "        outputs = model(imgs)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "#         print(predicted)\n",
    "#         print(\"\\n\")\n",
    "#         print(labels)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted==labels).sum())\n",
    "    print(\"Accuracy \", correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2967e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2\n",
    "import torch.nn as nn\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias=False)\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "62cc11fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class ResNet10(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.ResNetBlocks = nn.Sequential(*(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
    "        self.fc1 = nn.Linear(16 * 16 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.ResNetBlocks(out)\n",
    "        out = out.view(-1, 16 * 16 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f796e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda:0')\n",
    "train_loader = torch.utils.data.DataLoader(transformed_cifar10, batch_size=64, shuffle=True)\n",
    "\n",
    "model = ResNet10()\n",
    "model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "learning_rate = 3e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5ce4480f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.380295\n",
      "Epoch: 1, Loss: 1.534053\n",
      "Epoch: 2, Loss: 1.522577\n",
      "Epoch: 3, Loss: 1.108553\n",
      "Epoch: 4, Loss: 0.830498\n",
      "Epoch: 5, Loss: 1.134960\n",
      "Epoch: 6, Loss: 0.875195\n",
      "Epoch: 7, Loss: 0.962270\n",
      "Epoch: 8, Loss: 0.826196\n",
      "Epoch: 9, Loss: 0.816926\n",
      "Epoch: 10, Loss: 0.934372\n",
      "Epoch: 11, Loss: 1.005834\n",
      "Epoch: 12, Loss: 1.560953\n",
      "Epoch: 13, Loss: 0.384691\n",
      "Epoch: 14, Loss: 1.155713\n",
      "Epoch: 15, Loss: 0.883912\n",
      "Epoch: 16, Loss: 0.866532\n",
      "Epoch: 17, Loss: 0.759370\n",
      "Epoch: 18, Loss: 0.868821\n",
      "Epoch: 19, Loss: 0.997360\n",
      "Epoch: 20, Loss: 0.648042\n",
      "Epoch: 21, Loss: 0.841270\n",
      "Epoch: 22, Loss: 0.368993\n",
      "Epoch: 23, Loss: 0.815533\n",
      "Epoch: 24, Loss: 0.946853\n",
      "Epoch: 25, Loss: 0.451778\n",
      "Epoch: 26, Loss: 0.729739\n",
      "Epoch: 27, Loss: 0.728167\n",
      "Epoch: 28, Loss: 0.459222\n",
      "Epoch: 29, Loss: 0.848244\n",
      "Epoch: 30, Loss: 0.874603\n",
      "Epoch: 31, Loss: 0.883124\n",
      "Epoch: 32, Loss: 0.394543\n",
      "Epoch: 33, Loss: 0.528460\n",
      "Epoch: 34, Loss: 0.280844\n",
      "Epoch: 35, Loss: 0.585605\n",
      "Epoch: 36, Loss: 0.984623\n",
      "Epoch: 37, Loss: 0.662719\n",
      "Epoch: 38, Loss: 0.800092\n",
      "Epoch: 39, Loss: 0.422235\n",
      "Epoch: 40, Loss: 0.761066\n",
      "Epoch: 41, Loss: 0.978466\n",
      "Epoch: 42, Loss: 0.361117\n",
      "Epoch: 43, Loss: 0.944220\n",
      "Epoch: 44, Loss: 0.596822\n",
      "Epoch: 45, Loss: 0.702380\n",
      "Epoch: 46, Loss: 0.494088\n",
      "Epoch: 47, Loss: 0.754879\n",
      "Epoch: 48, Loss: 1.341822\n",
      "Epoch: 49, Loss: 0.843196\n",
      "Epoch: 50, Loss: 0.692155\n",
      "Epoch: 51, Loss: 0.495611\n",
      "Epoch: 52, Loss: 0.222088\n",
      "Epoch: 53, Loss: 0.703717\n",
      "Epoch: 54, Loss: 1.299031\n",
      "Epoch: 55, Loss: 0.772099\n",
      "Epoch: 56, Loss: 0.555656\n",
      "Epoch: 57, Loss: 0.949775\n",
      "Epoch: 58, Loss: 0.557609\n",
      "Epoch: 59, Loss: 1.331509\n",
      "Epoch: 60, Loss: 0.536458\n",
      "Epoch: 61, Loss: 0.465959\n",
      "Epoch: 62, Loss: 1.159207\n",
      "Epoch: 63, Loss: 0.631564\n",
      "Epoch: 64, Loss: 0.692085\n",
      "Epoch: 65, Loss: 0.599146\n",
      "Epoch: 66, Loss: 0.493434\n",
      "Epoch: 67, Loss: 0.681162\n",
      "Epoch: 68, Loss: 0.945495\n",
      "Epoch: 69, Loss: 0.317505\n",
      "Epoch: 70, Loss: 1.246249\n",
      "Epoch: 71, Loss: 0.554352\n",
      "Epoch: 72, Loss: 1.080858\n",
      "Epoch: 73, Loss: 0.322223\n",
      "Epoch: 74, Loss: 1.397937\n",
      "Epoch: 75, Loss: 0.558204\n",
      "Epoch: 76, Loss: 1.345222\n",
      "Epoch: 77, Loss: 0.284252\n",
      "Epoch: 78, Loss: 0.370521\n",
      "Epoch: 79, Loss: 0.449116\n",
      "Epoch: 80, Loss: 0.880541\n",
      "Epoch: 81, Loss: 0.267190\n",
      "Epoch: 82, Loss: 0.515562\n",
      "Epoch: 83, Loss: 0.834472\n",
      "Epoch: 84, Loss: 0.251172\n",
      "Epoch: 85, Loss: 0.347078\n",
      "Epoch: 86, Loss: 0.733065\n",
      "Epoch: 87, Loss: 0.691058\n",
      "Epoch: 88, Loss: 0.332971\n",
      "Epoch: 89, Loss: 0.771989\n",
      "Epoch: 90, Loss: 0.355433\n",
      "Epoch: 91, Loss: 0.210443\n",
      "Epoch: 92, Loss: 0.776835\n",
      "Epoch: 93, Loss: 0.366610\n",
      "Epoch: 94, Loss: 0.753169\n",
      "Epoch: 95, Loss: 0.511624\n",
      "Epoch: 96, Loss: 0.379635\n",
      "Epoch: 97, Loss: 0.367830\n",
      "Epoch: 98, Loss: 0.638768\n",
      "Epoch: 99, Loss: 1.086178\n",
      "Epoch: 100, Loss: 0.596224\n",
      "Epoch: 101, Loss: 1.402197\n",
      "Epoch: 102, Loss: 0.950927\n",
      "Epoch: 103, Loss: 0.580121\n",
      "Epoch: 104, Loss: 0.605557\n",
      "Epoch: 105, Loss: 0.667210\n",
      "Epoch: 106, Loss: 0.715795\n",
      "Epoch: 107, Loss: 0.540268\n",
      "Epoch: 108, Loss: 0.384047\n",
      "Epoch: 109, Loss: 0.218978\n",
      "Epoch: 110, Loss: 0.905232\n",
      "Epoch: 111, Loss: 0.808130\n",
      "Epoch: 112, Loss: 0.106361\n",
      "Epoch: 113, Loss: 0.463803\n",
      "Epoch: 114, Loss: 1.025647\n",
      "Epoch: 115, Loss: 0.613296\n",
      "Epoch: 116, Loss: 0.773259\n",
      "Epoch: 117, Loss: 0.230441\n",
      "Epoch: 118, Loss: 0.362173\n",
      "Epoch: 119, Loss: 0.227840\n",
      "Epoch: 120, Loss: 0.465431\n",
      "Epoch: 121, Loss: 1.140233\n",
      "Epoch: 122, Loss: 0.666230\n",
      "Epoch: 123, Loss: 0.449102\n",
      "Epoch: 124, Loss: 1.061858\n",
      "Epoch: 125, Loss: 0.562275\n",
      "Epoch: 126, Loss: 1.303998\n",
      "Epoch: 127, Loss: 0.315353\n",
      "Epoch: 128, Loss: 0.515697\n",
      "Epoch: 129, Loss: 0.409228\n",
      "Epoch: 130, Loss: 0.411534\n",
      "Epoch: 131, Loss: 1.004170\n",
      "Epoch: 132, Loss: 0.176068\n",
      "Epoch: 133, Loss: 0.411302\n",
      "Epoch: 134, Loss: 0.859111\n",
      "Epoch: 135, Loss: 1.019657\n",
      "Epoch: 136, Loss: 0.485963\n",
      "Epoch: 137, Loss: 0.321783\n",
      "Epoch: 138, Loss: 0.318216\n",
      "Epoch: 139, Loss: 0.284407\n",
      "Epoch: 140, Loss: 0.488205\n",
      "Epoch: 141, Loss: 1.084039\n",
      "Epoch: 142, Loss: 0.529898\n",
      "Epoch: 143, Loss: 0.150954\n",
      "Epoch: 144, Loss: 0.415855\n",
      "Epoch: 145, Loss: 0.247873\n",
      "Epoch: 146, Loss: 0.415892\n",
      "Epoch: 147, Loss: 0.495871\n",
      "Epoch: 148, Loss: 0.493417\n",
      "Epoch: 149, Loss: 0.656050\n",
      "Epoch: 150, Loss: 1.123717\n",
      "Epoch: 151, Loss: 0.385549\n",
      "Epoch: 152, Loss: 0.639023\n",
      "Epoch: 153, Loss: 1.584278\n",
      "Epoch: 154, Loss: 0.354603\n",
      "Epoch: 155, Loss: 0.330090\n",
      "Epoch: 156, Loss: 0.429722\n",
      "Epoch: 157, Loss: 0.632479\n",
      "Epoch: 158, Loss: 0.248089\n",
      "Epoch: 159, Loss: 0.332424\n",
      "Epoch: 160, Loss: 0.794612\n",
      "Epoch: 161, Loss: 0.615460\n",
      "Epoch: 162, Loss: 0.419100\n",
      "Epoch: 163, Loss: 0.269721\n",
      "Epoch: 164, Loss: 0.525057\n",
      "Epoch: 165, Loss: 0.285052\n",
      "Epoch: 166, Loss: 0.779525\n",
      "Epoch: 167, Loss: 0.204980\n",
      "Epoch: 168, Loss: 1.227870\n",
      "Epoch: 169, Loss: 0.718987\n",
      "Epoch: 170, Loss: 0.558725\n",
      "Epoch: 171, Loss: 0.328941\n",
      "Epoch: 172, Loss: 0.057317\n",
      "Epoch: 173, Loss: 0.287118\n",
      "Epoch: 174, Loss: 0.533803\n",
      "Epoch: 175, Loss: 0.427484\n",
      "Epoch: 176, Loss: 0.524672\n",
      "Epoch: 177, Loss: 1.293799\n",
      "Epoch: 178, Loss: 0.753082\n",
      "Epoch: 179, Loss: 0.210859\n",
      "Epoch: 180, Loss: 0.190630\n",
      "Epoch: 181, Loss: 0.659434\n",
      "Epoch: 182, Loss: 1.065244\n",
      "Epoch: 183, Loss: 0.738482\n",
      "Epoch: 184, Loss: 0.649073\n",
      "Epoch: 185, Loss: 0.554764\n",
      "Epoch: 186, Loss: 0.253485\n",
      "Epoch: 187, Loss: 0.287929\n",
      "Epoch: 188, Loss: 0.709250\n",
      "Epoch: 189, Loss: 0.530483\n",
      "Epoch: 190, Loss: 1.068691\n",
      "Epoch: 191, Loss: 0.288138\n",
      "Epoch: 192, Loss: 0.522530\n",
      "Epoch: 193, Loss: 0.754178\n",
      "Epoch: 194, Loss: 0.144283\n",
      "Epoch: 195, Loss: 0.854065\n",
      "Epoch: 196, Loss: 0.343788\n",
      "Epoch: 197, Loss: 0.407730\n",
      "Epoch: 198, Loss: 0.659166\n",
      "Epoch: 199, Loss: 0.984298\n",
      "Epoch: 200, Loss: 0.055736\n",
      "Epoch: 201, Loss: 1.379359\n",
      "Epoch: 202, Loss: 2.160732\n",
      "Epoch: 203, Loss: 0.166857\n",
      "Epoch: 204, Loss: 0.645467\n",
      "Epoch: 205, Loss: 0.663232\n",
      "Epoch: 206, Loss: 0.363973\n",
      "Epoch: 207, Loss: 0.454711\n",
      "Epoch: 208, Loss: 0.470384\n",
      "Epoch: 209, Loss: 0.649272\n",
      "Epoch: 210, Loss: 0.259131\n",
      "Epoch: 211, Loss: 0.682679\n",
      "Epoch: 212, Loss: 0.198253\n",
      "Epoch: 213, Loss: 0.265053\n",
      "Epoch: 214, Loss: 0.846186\n",
      "Epoch: 215, Loss: 0.492594\n",
      "Epoch: 216, Loss: 0.508281\n",
      "Epoch: 217, Loss: 0.109077\n",
      "Epoch: 218, Loss: 0.461323\n",
      "Epoch: 219, Loss: 0.880210\n",
      "Epoch: 220, Loss: 1.335081\n",
      "Epoch: 221, Loss: 0.448995\n",
      "Epoch: 222, Loss: 0.827476\n",
      "Epoch: 223, Loss: 0.520554\n",
      "Epoch: 224, Loss: 0.243618\n",
      "Epoch: 225, Loss: 0.280187\n",
      "Epoch: 226, Loss: 0.348860\n",
      "Epoch: 227, Loss: 1.193089\n",
      "Epoch: 228, Loss: 0.437516\n",
      "Epoch: 229, Loss: 0.912597\n",
      "Epoch: 230, Loss: 0.350146\n",
      "Epoch: 231, Loss: 0.357153\n",
      "Epoch: 232, Loss: 0.930340\n",
      "Epoch: 233, Loss: 1.110108\n",
      "Epoch: 234, Loss: 0.904539\n",
      "Epoch: 235, Loss: 0.595233\n",
      "Epoch: 236, Loss: 1.050200\n",
      "Epoch: 237, Loss: 0.680218\n",
      "Epoch: 238, Loss: 0.464325\n",
      "Epoch: 239, Loss: 0.656830\n",
      "Epoch: 240, Loss: 0.375612\n",
      "Epoch: 241, Loss: 0.760939\n",
      "Epoch: 242, Loss: 0.368319\n",
      "Epoch: 243, Loss: 0.333448\n",
      "Epoch: 244, Loss: 0.393145\n",
      "Epoch: 245, Loss: 1.114257\n",
      "Epoch: 246, Loss: 1.624054\n",
      "Epoch: 247, Loss: 0.087357\n",
      "Epoch: 248, Loss: 0.260697\n",
      "Epoch: 249, Loss: 1.418267\n",
      "Epoch: 250, Loss: 0.588495\n",
      "Epoch: 251, Loss: 1.753193\n",
      "Epoch: 252, Loss: 0.306724\n",
      "Epoch: 253, Loss: 0.372830\n",
      "Epoch: 254, Loss: 0.403171\n",
      "Epoch: 255, Loss: 0.408812\n",
      "Epoch: 256, Loss: 0.668673\n",
      "Epoch: 257, Loss: 0.597615\n",
      "Epoch: 258, Loss: 0.762671\n",
      "Epoch: 259, Loss: 0.030158\n",
      "Epoch: 260, Loss: 0.151636\n",
      "Epoch: 261, Loss: 0.428615\n",
      "Epoch: 262, Loss: 0.499738\n",
      "Epoch: 263, Loss: 0.731224\n",
      "Epoch: 264, Loss: 0.677875\n",
      "Epoch: 265, Loss: 0.277742\n",
      "Epoch: 266, Loss: 0.620186\n",
      "Epoch: 267, Loss: 0.808300\n",
      "Epoch: 268, Loss: 1.481186\n",
      "Epoch: 269, Loss: 1.378914\n",
      "Epoch: 270, Loss: 0.751533\n",
      "Epoch: 271, Loss: 0.126849\n",
      "Epoch: 272, Loss: 0.097029\n",
      "Epoch: 273, Loss: 0.837152\n",
      "Epoch: 274, Loss: 0.554728\n",
      "Epoch: 275, Loss: 0.562224\n",
      "Epoch: 276, Loss: 0.804801\n",
      "Epoch: 277, Loss: 0.250821\n",
      "Epoch: 278, Loss: 1.323176\n",
      "Epoch: 279, Loss: 0.245550\n",
      "Epoch: 280, Loss: 0.390145\n",
      "Epoch: 281, Loss: 0.809721\n",
      "Epoch: 282, Loss: 0.172840\n",
      "Epoch: 283, Loss: 0.641828\n",
      "Epoch: 284, Loss: 0.760464\n",
      "Epoch: 285, Loss: 0.963606\n",
      "Epoch: 286, Loss: 0.500598\n",
      "Epoch: 287, Loss: 0.623114\n",
      "Epoch: 288, Loss: 0.832079\n",
      "Epoch: 289, Loss: 1.291583\n",
      "Epoch: 290, Loss: 0.434085\n",
      "Epoch: 291, Loss: 0.546361\n",
      "Epoch: 292, Loss: 0.177244\n",
      "Epoch: 293, Loss: 0.202407\n",
      "Epoch: 294, Loss: 0.247989\n",
      "Epoch: 295, Loss: 0.487519\n",
      "Epoch: 296, Loss: 0.644584\n",
      "Epoch: 297, Loss: 0.389295\n",
      "Epoch: 298, Loss: 0.566370\n",
      "Epoch: 299, Loss: 0.682870\n",
      "3296.5056784152985\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 300\n",
    "training(model, optimizer, loss_function, n_epochs, device, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2be05e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational complexity:       0.02 GMac\n",
      "Number of parameters:           272.68 k\n"
     ]
    }
   ],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "\n",
    "macs, params = get_model_complexity_info(model, ( 3, 32,32), as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "15df9199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  0.1\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(val_transformed_cifar10, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size=imgs.shape[0]\n",
    "        outputs = model(imgs)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "#         print(predicted)\n",
    "#         print(\"\\n\")\n",
    "#         print(labels)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted==labels).sum())\n",
    "    print(\"Accuracy \", correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b2af2a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 2B\n",
    "\n",
    "import time\n",
    "def training(model, optimizer, loss_fn, n_epochs, device, train_loader, l2_lambda):\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            batch_size = imgs.shape[0]\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn((outputs), labels)\n",
    "            l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "            loss = loss + l2_lambda* l2_norm\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3480722c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 10.951440\n",
      "Epoch: 1, Loss: 4.647394\n",
      "Epoch: 2, Loss: 2.800955\n",
      "Epoch: 3, Loss: 2.225659\n",
      "Epoch: 4, Loss: 2.161125\n",
      "Epoch: 5, Loss: 1.862766\n",
      "Epoch: 6, Loss: 1.583517\n",
      "Epoch: 7, Loss: 2.222004\n",
      "Epoch: 8, Loss: 1.820010\n",
      "Epoch: 9, Loss: 1.967250\n",
      "Epoch: 10, Loss: 2.207386\n",
      "Epoch: 11, Loss: 1.357456\n",
      "Epoch: 12, Loss: 1.738413\n",
      "Epoch: 13, Loss: 1.820579\n",
      "Epoch: 14, Loss: 1.710661\n",
      "Epoch: 15, Loss: 1.451937\n",
      "Epoch: 16, Loss: 1.800370\n",
      "Epoch: 17, Loss: 1.454486\n",
      "Epoch: 18, Loss: 1.609495\n",
      "Epoch: 19, Loss: 1.340552\n",
      "Epoch: 20, Loss: 1.648403\n",
      "Epoch: 21, Loss: 1.213099\n",
      "Epoch: 22, Loss: 1.931675\n",
      "Epoch: 23, Loss: 1.811163\n",
      "Epoch: 24, Loss: 1.763996\n",
      "Epoch: 25, Loss: 1.617107\n",
      "Epoch: 26, Loss: 1.084703\n",
      "Epoch: 27, Loss: 1.235137\n",
      "Epoch: 28, Loss: 1.430830\n",
      "Epoch: 29, Loss: 2.334033\n",
      "Epoch: 30, Loss: 1.967194\n",
      "Epoch: 31, Loss: 1.908156\n",
      "Epoch: 32, Loss: 1.707417\n",
      "Epoch: 33, Loss: 1.488826\n",
      "Epoch: 34, Loss: 1.145470\n",
      "Epoch: 35, Loss: 1.285830\n",
      "Epoch: 36, Loss: 1.456136\n",
      "Epoch: 37, Loss: 1.784907\n",
      "Epoch: 38, Loss: 1.521228\n",
      "Epoch: 39, Loss: 1.845839\n",
      "Epoch: 40, Loss: 1.705061\n",
      "Epoch: 41, Loss: 1.698319\n",
      "Epoch: 42, Loss: 1.780094\n",
      "Epoch: 43, Loss: 1.521727\n",
      "Epoch: 44, Loss: 1.572895\n",
      "Epoch: 45, Loss: 1.475441\n",
      "Epoch: 46, Loss: 1.538025\n",
      "Epoch: 47, Loss: 1.333474\n",
      "Epoch: 48, Loss: 1.578037\n",
      "Epoch: 49, Loss: 1.451524\n",
      "Epoch: 50, Loss: 1.436068\n",
      "Epoch: 51, Loss: 0.996346\n",
      "Epoch: 52, Loss: 1.321064\n",
      "Epoch: 53, Loss: 1.198278\n",
      "Epoch: 54, Loss: 1.344646\n",
      "Epoch: 55, Loss: 1.375412\n",
      "Epoch: 56, Loss: 1.390983\n",
      "Epoch: 57, Loss: 1.547436\n",
      "Epoch: 58, Loss: 1.510586\n",
      "Epoch: 59, Loss: 1.208568\n",
      "Epoch: 60, Loss: 1.203075\n",
      "Epoch: 61, Loss: 1.943881\n",
      "Epoch: 62, Loss: 1.362887\n",
      "Epoch: 63, Loss: 1.667814\n",
      "Epoch: 64, Loss: 1.431761\n",
      "Epoch: 65, Loss: 1.401344\n",
      "Epoch: 66, Loss: 1.655278\n",
      "Epoch: 67, Loss: 1.185472\n",
      "Epoch: 68, Loss: 1.493755\n",
      "Epoch: 69, Loss: 1.803148\n",
      "Epoch: 70, Loss: 1.430788\n",
      "Epoch: 71, Loss: 1.498374\n",
      "Epoch: 72, Loss: 1.565392\n",
      "Epoch: 73, Loss: 1.876347\n",
      "Epoch: 74, Loss: 1.605134\n",
      "Epoch: 75, Loss: 1.304337\n",
      "Epoch: 76, Loss: 1.166213\n",
      "Epoch: 77, Loss: 1.801992\n",
      "Epoch: 78, Loss: 1.346815\n",
      "Epoch: 79, Loss: 1.754570\n",
      "Epoch: 80, Loss: 1.262118\n",
      "Epoch: 81, Loss: 1.594926\n",
      "Epoch: 82, Loss: 1.379508\n",
      "Epoch: 83, Loss: 1.905716\n",
      "Epoch: 84, Loss: 1.546248\n",
      "Epoch: 85, Loss: 1.469356\n",
      "Epoch: 86, Loss: 1.541683\n",
      "Epoch: 87, Loss: 1.653567\n",
      "Epoch: 88, Loss: 1.279009\n",
      "Epoch: 89, Loss: 1.289350\n",
      "Epoch: 90, Loss: 1.771722\n",
      "Epoch: 91, Loss: 1.597301\n",
      "Epoch: 92, Loss: 1.254401\n",
      "Epoch: 93, Loss: 1.243417\n",
      "Epoch: 94, Loss: 1.182143\n",
      "Epoch: 95, Loss: 1.399967\n",
      "Epoch: 96, Loss: 1.461592\n",
      "Epoch: 97, Loss: 1.601644\n",
      "Epoch: 98, Loss: 1.570920\n",
      "Epoch: 99, Loss: 1.374875\n",
      "Epoch: 100, Loss: 1.627290\n",
      "Epoch: 101, Loss: 1.276383\n",
      "Epoch: 102, Loss: 1.752806\n",
      "Epoch: 103, Loss: 1.550049\n",
      "Epoch: 104, Loss: 1.343803\n",
      "Epoch: 105, Loss: 1.420080\n",
      "Epoch: 106, Loss: 1.926564\n",
      "Epoch: 107, Loss: 1.481239\n",
      "Epoch: 108, Loss: 1.833448\n",
      "Epoch: 109, Loss: 1.469544\n",
      "Epoch: 110, Loss: 1.259187\n",
      "Epoch: 111, Loss: 1.662703\n",
      "Epoch: 112, Loss: 1.338640\n",
      "Epoch: 113, Loss: 2.187330\n",
      "Epoch: 114, Loss: 2.158751\n",
      "Epoch: 115, Loss: 1.472168\n",
      "Epoch: 116, Loss: 1.559178\n",
      "Epoch: 117, Loss: 2.076247\n",
      "Epoch: 118, Loss: 1.676108\n",
      "Epoch: 119, Loss: 1.110409\n",
      "Epoch: 120, Loss: 1.289116\n",
      "Epoch: 121, Loss: 1.369417\n",
      "Epoch: 122, Loss: 1.616939\n",
      "Epoch: 123, Loss: 1.760504\n",
      "Epoch: 124, Loss: 1.690705\n",
      "Epoch: 125, Loss: 1.244495\n",
      "Epoch: 126, Loss: 1.286316\n",
      "Epoch: 127, Loss: 1.418093\n",
      "Epoch: 128, Loss: 1.628275\n",
      "Epoch: 129, Loss: 1.500955\n",
      "Epoch: 130, Loss: 1.162256\n",
      "Epoch: 131, Loss: 1.317757\n",
      "Epoch: 132, Loss: 1.796748\n",
      "Epoch: 133, Loss: 1.024727\n",
      "Epoch: 134, Loss: 1.348021\n",
      "Epoch: 135, Loss: 1.352397\n",
      "Epoch: 136, Loss: 1.374675\n",
      "Epoch: 137, Loss: 1.262214\n",
      "Epoch: 138, Loss: 1.797882\n",
      "Epoch: 139, Loss: 1.275564\n",
      "Epoch: 140, Loss: 1.452496\n",
      "Epoch: 141, Loss: 1.616353\n",
      "Epoch: 142, Loss: 1.674169\n",
      "Epoch: 143, Loss: 1.384351\n",
      "Epoch: 144, Loss: 1.395098\n",
      "Epoch: 145, Loss: 1.692765\n",
      "Epoch: 146, Loss: 1.141497\n",
      "Epoch: 147, Loss: 1.567030\n",
      "Epoch: 148, Loss: 1.460711\n",
      "Epoch: 149, Loss: 1.191043\n",
      "Epoch: 150, Loss: 1.356780\n",
      "Epoch: 151, Loss: 1.553092\n",
      "Epoch: 152, Loss: 1.711885\n",
      "Epoch: 153, Loss: 1.393295\n",
      "Epoch: 154, Loss: 1.509276\n",
      "Epoch: 155, Loss: 1.445006\n",
      "Epoch: 156, Loss: 1.536040\n",
      "Epoch: 157, Loss: 1.182578\n",
      "Epoch: 158, Loss: 1.224824\n",
      "Epoch: 159, Loss: 1.398164\n",
      "Epoch: 160, Loss: 1.022205\n",
      "Epoch: 161, Loss: 1.629232\n",
      "Epoch: 162, Loss: 1.157836\n",
      "Epoch: 163, Loss: 1.522558\n",
      "Epoch: 164, Loss: 1.416380\n",
      "Epoch: 165, Loss: 1.139839\n",
      "Epoch: 166, Loss: 1.407486\n",
      "Epoch: 167, Loss: 1.403350\n",
      "Epoch: 168, Loss: 2.084922\n",
      "Epoch: 169, Loss: 1.414070\n",
      "Epoch: 170, Loss: 1.167084\n",
      "Epoch: 171, Loss: 1.256077\n",
      "Epoch: 172, Loss: 1.405317\n",
      "Epoch: 173, Loss: 1.585081\n",
      "Epoch: 174, Loss: 1.205439\n",
      "Epoch: 175, Loss: 0.914562\n",
      "Epoch: 176, Loss: 1.597933\n",
      "Epoch: 177, Loss: 1.265217\n",
      "Epoch: 178, Loss: 1.070268\n",
      "Epoch: 179, Loss: 1.284259\n",
      "Epoch: 180, Loss: 1.493295\n",
      "Epoch: 181, Loss: 1.650852\n",
      "Epoch: 182, Loss: 1.669943\n",
      "Epoch: 183, Loss: 1.203025\n",
      "Epoch: 184, Loss: 1.286689\n",
      "Epoch: 185, Loss: 1.103081\n",
      "Epoch: 186, Loss: 1.475908\n",
      "Epoch: 187, Loss: 1.200938\n",
      "Epoch: 188, Loss: 2.041421\n",
      "Epoch: 189, Loss: 1.208506\n",
      "Epoch: 190, Loss: 1.568819\n",
      "Epoch: 191, Loss: 1.304908\n",
      "Epoch: 192, Loss: 1.618936\n",
      "Epoch: 193, Loss: 1.374373\n",
      "Epoch: 194, Loss: 1.343892\n",
      "Epoch: 195, Loss: 0.999015\n",
      "Epoch: 196, Loss: 1.532312\n",
      "Epoch: 197, Loss: 1.981481\n",
      "Epoch: 198, Loss: 1.471740\n",
      "Epoch: 199, Loss: 1.551695\n",
      "Epoch: 200, Loss: 1.424155\n",
      "Epoch: 201, Loss: 1.567466\n",
      "Epoch: 202, Loss: 1.498433\n",
      "Epoch: 203, Loss: 1.241199\n",
      "Epoch: 204, Loss: 1.980607\n",
      "Epoch: 205, Loss: 1.659364\n",
      "Epoch: 206, Loss: 1.243863\n",
      "Epoch: 207, Loss: 1.782366\n",
      "Epoch: 208, Loss: 0.993867\n",
      "Epoch: 209, Loss: 1.186438\n",
      "Epoch: 210, Loss: 1.961484\n",
      "Epoch: 211, Loss: 1.464996\n",
      "Epoch: 212, Loss: 1.462134\n",
      "Epoch: 213, Loss: 1.528311\n",
      "Epoch: 214, Loss: 1.373449\n",
      "Epoch: 215, Loss: 1.661284\n",
      "Epoch: 216, Loss: 1.714193\n",
      "Epoch: 217, Loss: 1.325982\n",
      "Epoch: 218, Loss: 1.585445\n",
      "Epoch: 219, Loss: 1.252668\n",
      "Epoch: 220, Loss: 1.697926\n",
      "Epoch: 221, Loss: 1.716879\n",
      "Epoch: 222, Loss: 1.265929\n",
      "Epoch: 223, Loss: 1.222662\n",
      "Epoch: 224, Loss: 1.691228\n",
      "Epoch: 225, Loss: 1.460888\n",
      "Epoch: 226, Loss: 1.316936\n",
      "Epoch: 227, Loss: 1.205209\n",
      "Epoch: 228, Loss: 1.040003\n",
      "Epoch: 229, Loss: 1.535553\n",
      "Epoch: 230, Loss: 2.005908\n",
      "Epoch: 231, Loss: 1.772653\n",
      "Epoch: 232, Loss: 1.402008\n",
      "Epoch: 233, Loss: 1.206209\n",
      "Epoch: 234, Loss: 1.277411\n",
      "Epoch: 235, Loss: 1.304767\n",
      "Epoch: 236, Loss: 1.241860\n",
      "Epoch: 237, Loss: 1.222328\n",
      "Epoch: 238, Loss: 1.531687\n",
      "Epoch: 239, Loss: 1.206513\n",
      "Epoch: 240, Loss: 1.273515\n",
      "Epoch: 241, Loss: 1.677120\n",
      "Epoch: 242, Loss: 1.482058\n",
      "Epoch: 243, Loss: 1.442981\n",
      "Epoch: 244, Loss: 1.362632\n",
      "Epoch: 245, Loss: 1.430273\n",
      "Epoch: 246, Loss: 1.685722\n",
      "Epoch: 247, Loss: 1.389421\n",
      "Epoch: 248, Loss: 1.456757\n",
      "Epoch: 249, Loss: 1.518691\n",
      "Epoch: 250, Loss: 1.511637\n",
      "Epoch: 251, Loss: 1.814562\n",
      "Epoch: 252, Loss: 1.442732\n",
      "Epoch: 253, Loss: 1.977664\n",
      "Epoch: 254, Loss: 1.585663\n",
      "Epoch: 255, Loss: 1.716352\n",
      "Epoch: 256, Loss: 1.832749\n",
      "Epoch: 257, Loss: 0.968053\n",
      "Epoch: 258, Loss: 2.134270\n",
      "Epoch: 259, Loss: 1.772189\n",
      "Epoch: 260, Loss: 1.343420\n",
      "Epoch: 261, Loss: 1.490398\n",
      "Epoch: 262, Loss: 2.132844\n",
      "Epoch: 263, Loss: 1.196839\n",
      "Epoch: 264, Loss: 1.784396\n",
      "Epoch: 265, Loss: 1.505852\n",
      "Epoch: 266, Loss: 1.551780\n",
      "Epoch: 267, Loss: 1.474594\n",
      "Epoch: 268, Loss: 1.533204\n",
      "Epoch: 269, Loss: 1.299223\n",
      "Epoch: 270, Loss: 1.888834\n",
      "Epoch: 271, Loss: 1.288868\n",
      "Epoch: 272, Loss: 1.426370\n",
      "Epoch: 273, Loss: 1.314751\n",
      "Epoch: 274, Loss: 0.965519\n",
      "Epoch: 275, Loss: 1.622324\n",
      "Epoch: 276, Loss: 1.654595\n",
      "Epoch: 277, Loss: 1.545431\n",
      "Epoch: 278, Loss: 1.257667\n",
      "Epoch: 279, Loss: 1.529096\n",
      "Epoch: 280, Loss: 1.988781\n",
      "Epoch: 281, Loss: 1.947571\n",
      "Epoch: 282, Loss: 1.140765\n",
      "Epoch: 283, Loss: 1.313967\n",
      "Epoch: 284, Loss: 1.178080\n",
      "Epoch: 285, Loss: 1.361185\n",
      "Epoch: 286, Loss: 1.345096\n",
      "Epoch: 287, Loss: 1.243309\n",
      "Epoch: 288, Loss: 2.127151\n",
      "Epoch: 289, Loss: 1.609862\n",
      "Epoch: 290, Loss: 1.359417\n",
      "Epoch: 291, Loss: 1.526035\n",
      "Epoch: 292, Loss: 1.822676\n",
      "Epoch: 293, Loss: 1.628549\n",
      "Epoch: 294, Loss: 1.414407\n",
      "Epoch: 295, Loss: 2.173963\n",
      "Epoch: 296, Loss: 1.359991\n",
      "Epoch: 297, Loss: 1.186405\n",
      "Epoch: 298, Loss: 1.215213\n",
      "Epoch: 299, Loss: 1.284183\n",
      "3420.2066872119904\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 300\n",
    "l2_lambda = .001\n",
    "training(model, optimizer, loss_function, n_epochs, device, train_loader, l2_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "600d3d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: variables __flops__ or __params__ are already defined for the moduleConv2d ptflops can affect your code!\n",
      "Warning: variables __flops__ or __params__ are already defined for the moduleConv2d ptflops can affect your code!\n",
      "Warning: variables __flops__ or __params__ are already defined for the moduleBatchNorm2d ptflops can affect your code!\n",
      "Warning: variables __flops__ or __params__ are already defined for the moduleLinear ptflops can affect your code!\n",
      "Warning: variables __flops__ or __params__ are already defined for the moduleLinear ptflops can affect your code!\n",
      "Computational complexity:       0.02 GMac\n",
      "Number of parameters:           272.68 k\n"
     ]
    }
   ],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "\n",
    "macs, params = get_model_complexity_info(model, ( 3, 32,32), as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e3d8010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  0.5052\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(val_transformed_cifar10, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size=imgs.shape[0]\n",
    "        outputs = model(imgs)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "#         print(predicted)\n",
    "#         print(\"\\n\")\n",
    "#         print(labels)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted==labels).sum())\n",
    "    print(\"Accuracy \", correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9cd2cb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class ResNet10(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_dropout = nn.Dropout2d(p=0.3)\n",
    "        self.ResNetBlocks = nn.Sequential(*(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
    "        self.fc1 = nn.Linear(16 * 16 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.conv1_dropout(out)\n",
    "        out = self.ResNetBlocks(out)\n",
    "        out = out.view(-1, 16 * 16 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "973b3352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda:0')\n",
    "train_loader = torch.utils.data.DataLoader(transformed_cifar10, batch_size=64, shuffle=True)\n",
    "\n",
    "model = ResNet10()\n",
    "model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "learning_rate = 3e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "75a32a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.109835\n",
      "Epoch: 1, Loss: 1.969299\n",
      "Epoch: 2, Loss: 1.010406\n",
      "Epoch: 3, Loss: 1.695881\n",
      "Epoch: 4, Loss: 1.560346\n",
      "Epoch: 5, Loss: 1.402911\n",
      "Epoch: 6, Loss: 0.806718\n",
      "Epoch: 7, Loss: 1.900599\n",
      "Epoch: 8, Loss: 1.594776\n",
      "Epoch: 9, Loss: 1.694569\n",
      "Epoch: 10, Loss: 1.355907\n",
      "Epoch: 11, Loss: 1.411947\n",
      "Epoch: 12, Loss: 1.280041\n",
      "Epoch: 13, Loss: 1.848790\n",
      "Epoch: 14, Loss: 1.218277\n",
      "Epoch: 15, Loss: 1.878471\n",
      "Epoch: 16, Loss: 1.468370\n",
      "Epoch: 17, Loss: 1.517360\n",
      "Epoch: 18, Loss: 0.981155\n",
      "Epoch: 19, Loss: 1.447148\n",
      "Epoch: 20, Loss: 1.244213\n",
      "Epoch: 21, Loss: 1.141342\n",
      "Epoch: 22, Loss: 1.386391\n",
      "Epoch: 23, Loss: 1.805052\n",
      "Epoch: 24, Loss: 1.588807\n",
      "Epoch: 25, Loss: 2.014440\n",
      "Epoch: 26, Loss: 1.912367\n",
      "Epoch: 27, Loss: 1.130355\n",
      "Epoch: 28, Loss: 1.580149\n",
      "Epoch: 29, Loss: 1.610882\n",
      "Epoch: 30, Loss: 1.176360\n",
      "Epoch: 31, Loss: 1.310100\n",
      "Epoch: 32, Loss: 1.648198\n",
      "Epoch: 33, Loss: 1.367503\n",
      "Epoch: 34, Loss: 1.422516\n",
      "Epoch: 35, Loss: 1.680829\n",
      "Epoch: 36, Loss: 1.376226\n",
      "Epoch: 37, Loss: 1.313788\n",
      "Epoch: 38, Loss: 1.669272\n",
      "Epoch: 39, Loss: 1.532687\n",
      "Epoch: 40, Loss: 1.132806\n",
      "Epoch: 41, Loss: 1.431460\n",
      "Epoch: 42, Loss: 1.331438\n",
      "Epoch: 43, Loss: 1.178844\n",
      "Epoch: 44, Loss: 0.870595\n",
      "Epoch: 45, Loss: 1.118547\n",
      "Epoch: 46, Loss: 1.479602\n",
      "Epoch: 47, Loss: 1.420517\n",
      "Epoch: 48, Loss: 1.183153\n",
      "Epoch: 49, Loss: 0.880000\n",
      "Epoch: 50, Loss: 1.091831\n",
      "Epoch: 51, Loss: 0.771163\n",
      "Epoch: 52, Loss: 1.255601\n",
      "Epoch: 53, Loss: 1.547179\n",
      "Epoch: 54, Loss: 1.380214\n",
      "Epoch: 55, Loss: 1.583119\n",
      "Epoch: 56, Loss: 2.063123\n",
      "Epoch: 57, Loss: 1.509463\n",
      "Epoch: 58, Loss: 1.347073\n",
      "Epoch: 59, Loss: 1.602739\n",
      "Epoch: 60, Loss: 1.518044\n",
      "Epoch: 61, Loss: 1.236717\n",
      "Epoch: 62, Loss: 1.634024\n",
      "Epoch: 63, Loss: 1.215928\n",
      "Epoch: 64, Loss: 1.501999\n",
      "Epoch: 65, Loss: 1.213793\n",
      "Epoch: 66, Loss: 1.092679\n",
      "Epoch: 67, Loss: 1.476160\n",
      "Epoch: 68, Loss: 1.120238\n",
      "Epoch: 69, Loss: 1.734018\n",
      "Epoch: 70, Loss: 1.145930\n",
      "Epoch: 71, Loss: 1.509084\n",
      "Epoch: 72, Loss: 1.183794\n",
      "Epoch: 73, Loss: 1.915933\n",
      "Epoch: 74, Loss: 1.407423\n",
      "Epoch: 75, Loss: 1.564566\n",
      "Epoch: 76, Loss: 1.179301\n",
      "Epoch: 77, Loss: 1.385219\n",
      "Epoch: 78, Loss: 1.373884\n",
      "Epoch: 79, Loss: 0.790598\n",
      "Epoch: 80, Loss: 1.290121\n",
      "Epoch: 81, Loss: 1.340641\n",
      "Epoch: 82, Loss: 1.470451\n",
      "Epoch: 83, Loss: 1.253623\n",
      "Epoch: 84, Loss: 1.728054\n",
      "Epoch: 85, Loss: 1.188489\n",
      "Epoch: 86, Loss: 1.468468\n",
      "Epoch: 87, Loss: 1.127864\n",
      "Epoch: 88, Loss: 1.158275\n",
      "Epoch: 89, Loss: 1.346945\n",
      "Epoch: 90, Loss: 1.487638\n",
      "Epoch: 91, Loss: 1.399106\n",
      "Epoch: 92, Loss: 1.279044\n",
      "Epoch: 93, Loss: 1.052002\n",
      "Epoch: 94, Loss: 0.909184\n",
      "Epoch: 95, Loss: 1.250015\n",
      "Epoch: 96, Loss: 1.938959\n",
      "Epoch: 97, Loss: 0.815221\n",
      "Epoch: 98, Loss: 1.646101\n",
      "Epoch: 99, Loss: 1.135005\n",
      "Epoch: 100, Loss: 1.137235\n",
      "Epoch: 101, Loss: 1.173342\n",
      "Epoch: 102, Loss: 1.185027\n",
      "Epoch: 103, Loss: 1.522191\n",
      "Epoch: 104, Loss: 1.248118\n",
      "Epoch: 105, Loss: 1.270838\n",
      "Epoch: 106, Loss: 1.597983\n",
      "Epoch: 107, Loss: 1.039197\n",
      "Epoch: 108, Loss: 1.122006\n",
      "Epoch: 109, Loss: 1.330083\n",
      "Epoch: 110, Loss: 1.480005\n",
      "Epoch: 111, Loss: 1.080469\n",
      "Epoch: 112, Loss: 1.274531\n",
      "Epoch: 113, Loss: 0.971847\n",
      "Epoch: 114, Loss: 1.260521\n",
      "Epoch: 115, Loss: 1.541933\n",
      "Epoch: 116, Loss: 1.061544\n",
      "Epoch: 117, Loss: 1.525583\n",
      "Epoch: 118, Loss: 1.332189\n",
      "Epoch: 119, Loss: 1.221895\n",
      "Epoch: 120, Loss: 0.968208\n",
      "Epoch: 121, Loss: 1.353856\n",
      "Epoch: 122, Loss: 1.041356\n",
      "Epoch: 123, Loss: 0.954831\n",
      "Epoch: 124, Loss: 0.986697\n",
      "Epoch: 125, Loss: 1.223044\n",
      "Epoch: 126, Loss: 0.969452\n",
      "Epoch: 127, Loss: 1.212687\n",
      "Epoch: 128, Loss: 1.454601\n",
      "Epoch: 129, Loss: 1.529407\n",
      "Epoch: 130, Loss: 1.328114\n",
      "Epoch: 131, Loss: 1.505339\n",
      "Epoch: 132, Loss: 1.812648\n",
      "Epoch: 133, Loss: 1.285132\n",
      "Epoch: 134, Loss: 2.102280\n",
      "Epoch: 135, Loss: 1.088109\n",
      "Epoch: 136, Loss: 1.301905\n",
      "Epoch: 137, Loss: 1.242243\n",
      "Epoch: 138, Loss: 1.954775\n",
      "Epoch: 139, Loss: 1.343727\n",
      "Epoch: 140, Loss: 0.998247\n",
      "Epoch: 141, Loss: 0.811761\n",
      "Epoch: 142, Loss: 1.386384\n",
      "Epoch: 143, Loss: 1.136151\n",
      "Epoch: 144, Loss: 1.094962\n",
      "Epoch: 145, Loss: 1.735127\n",
      "Epoch: 146, Loss: 1.389385\n",
      "Epoch: 147, Loss: 1.093279\n",
      "Epoch: 148, Loss: 1.628156\n",
      "Epoch: 149, Loss: 1.913773\n",
      "Epoch: 150, Loss: 0.941347\n",
      "Epoch: 151, Loss: 1.662392\n",
      "Epoch: 152, Loss: 1.674738\n",
      "Epoch: 153, Loss: 1.326700\n",
      "Epoch: 154, Loss: 1.376500\n",
      "Epoch: 155, Loss: 1.489647\n",
      "Epoch: 156, Loss: 0.778387\n",
      "Epoch: 157, Loss: 1.237298\n",
      "Epoch: 158, Loss: 1.619611\n",
      "Epoch: 159, Loss: 1.182442\n",
      "Epoch: 160, Loss: 1.477567\n",
      "Epoch: 161, Loss: 1.383962\n",
      "Epoch: 162, Loss: 1.603646\n",
      "Epoch: 163, Loss: 1.462832\n",
      "Epoch: 164, Loss: 0.936300\n",
      "Epoch: 165, Loss: 1.054512\n",
      "Epoch: 166, Loss: 1.039273\n",
      "Epoch: 167, Loss: 1.412014\n",
      "Epoch: 168, Loss: 1.179628\n",
      "Epoch: 169, Loss: 1.377108\n",
      "Epoch: 170, Loss: 1.263451\n",
      "Epoch: 171, Loss: 1.265061\n",
      "Epoch: 172, Loss: 0.842233\n",
      "Epoch: 173, Loss: 1.277047\n",
      "Epoch: 174, Loss: 1.631402\n",
      "Epoch: 175, Loss: 1.294199\n",
      "Epoch: 176, Loss: 1.347221\n",
      "Epoch: 177, Loss: 0.980425\n",
      "Epoch: 178, Loss: 1.452324\n",
      "Epoch: 179, Loss: 1.372367\n",
      "Epoch: 180, Loss: 1.193663\n",
      "Epoch: 181, Loss: 1.599889\n",
      "Epoch: 182, Loss: 1.442709\n",
      "Epoch: 183, Loss: 1.431270\n",
      "Epoch: 184, Loss: 1.603526\n",
      "Epoch: 185, Loss: 1.297375\n",
      "Epoch: 186, Loss: 1.476580\n",
      "Epoch: 187, Loss: 0.599081\n",
      "Epoch: 188, Loss: 1.633477\n",
      "Epoch: 189, Loss: 1.531051\n",
      "Epoch: 190, Loss: 1.644556\n",
      "Epoch: 191, Loss: 1.040234\n",
      "Epoch: 192, Loss: 1.111907\n",
      "Epoch: 193, Loss: 1.339648\n",
      "Epoch: 194, Loss: 1.118119\n",
      "Epoch: 195, Loss: 1.031549\n",
      "Epoch: 196, Loss: 0.994083\n",
      "Epoch: 197, Loss: 1.013652\n",
      "Epoch: 198, Loss: 1.638794\n",
      "Epoch: 199, Loss: 0.999518\n",
      "Epoch: 200, Loss: 1.249751\n",
      "Epoch: 201, Loss: 1.151665\n",
      "Epoch: 202, Loss: 1.548531\n",
      "Epoch: 203, Loss: 1.376494\n",
      "Epoch: 204, Loss: 1.067082\n",
      "Epoch: 205, Loss: 1.049309\n",
      "Epoch: 206, Loss: 1.796296\n",
      "Epoch: 207, Loss: 1.446187\n",
      "Epoch: 208, Loss: 1.177490\n",
      "Epoch: 209, Loss: 1.031091\n",
      "Epoch: 210, Loss: 1.066871\n",
      "Epoch: 211, Loss: 1.884002\n",
      "Epoch: 212, Loss: 1.110114\n",
      "Epoch: 213, Loss: 1.568319\n",
      "Epoch: 214, Loss: 1.074750\n",
      "Epoch: 215, Loss: 1.125444\n",
      "Epoch: 216, Loss: 1.228867\n",
      "Epoch: 217, Loss: 1.397890\n",
      "Epoch: 218, Loss: 1.406899\n",
      "Epoch: 219, Loss: 1.507887\n",
      "Epoch: 220, Loss: 1.587396\n",
      "Epoch: 221, Loss: 0.914172\n",
      "Epoch: 222, Loss: 1.340848\n",
      "Epoch: 223, Loss: 1.132545\n",
      "Epoch: 224, Loss: 1.208206\n",
      "Epoch: 225, Loss: 1.443970\n",
      "Epoch: 226, Loss: 1.755221\n",
      "Epoch: 227, Loss: 1.952862\n",
      "Epoch: 228, Loss: 1.550981\n",
      "Epoch: 229, Loss: 1.101821\n",
      "Epoch: 230, Loss: 0.943626\n",
      "Epoch: 231, Loss: 1.144663\n",
      "Epoch: 232, Loss: 1.095440\n",
      "Epoch: 233, Loss: 1.313185\n",
      "Epoch: 234, Loss: 1.364339\n",
      "Epoch: 235, Loss: 1.315221\n",
      "Epoch: 236, Loss: 1.641995\n",
      "Epoch: 237, Loss: 0.912288\n",
      "Epoch: 238, Loss: 1.330724\n",
      "Epoch: 239, Loss: 1.219863\n",
      "Epoch: 240, Loss: 1.508999\n",
      "Epoch: 241, Loss: 1.090021\n",
      "Epoch: 242, Loss: 0.967507\n",
      "Epoch: 243, Loss: 1.563182\n",
      "Epoch: 244, Loss: 0.896861\n",
      "Epoch: 245, Loss: 1.356174\n",
      "Epoch: 246, Loss: 1.422906\n",
      "Epoch: 247, Loss: 1.248907\n",
      "Epoch: 248, Loss: 1.101489\n",
      "Epoch: 249, Loss: 0.914826\n",
      "Epoch: 250, Loss: 1.269861\n",
      "Epoch: 251, Loss: 1.207112\n",
      "Epoch: 252, Loss: 1.068681\n",
      "Epoch: 253, Loss: 1.244215\n",
      "Epoch: 254, Loss: 0.994129\n",
      "Epoch: 255, Loss: 1.568863\n",
      "Epoch: 256, Loss: 1.565098\n",
      "Epoch: 257, Loss: 0.835318\n",
      "Epoch: 258, Loss: 1.050843\n",
      "Epoch: 259, Loss: 1.115909\n",
      "Epoch: 260, Loss: 1.590399\n",
      "Epoch: 261, Loss: 1.007654\n",
      "Epoch: 262, Loss: 1.444347\n",
      "Epoch: 263, Loss: 1.025030\n",
      "Epoch: 264, Loss: 1.234481\n",
      "Epoch: 265, Loss: 0.971877\n",
      "Epoch: 266, Loss: 2.059131\n",
      "Epoch: 267, Loss: 1.163352\n",
      "Epoch: 268, Loss: 0.956094\n",
      "Epoch: 269, Loss: 1.457070\n",
      "Epoch: 270, Loss: 0.743331\n",
      "Epoch: 271, Loss: 1.233499\n",
      "Epoch: 272, Loss: 1.804929\n",
      "Epoch: 273, Loss: 1.001853\n",
      "Epoch: 274, Loss: 0.890160\n",
      "Epoch: 275, Loss: 1.881496\n",
      "Epoch: 276, Loss: 1.298632\n",
      "Epoch: 277, Loss: 1.216773\n",
      "Epoch: 278, Loss: 1.150815\n",
      "Epoch: 279, Loss: 1.744982\n",
      "Epoch: 280, Loss: 1.267296\n",
      "Epoch: 281, Loss: 0.864310\n",
      "Epoch: 282, Loss: 1.406229\n",
      "Epoch: 283, Loss: 1.300003\n",
      "Epoch: 284, Loss: 1.054250\n",
      "Epoch: 285, Loss: 1.160117\n",
      "Epoch: 286, Loss: 1.144608\n",
      "Epoch: 287, Loss: 1.161726\n",
      "Epoch: 288, Loss: 1.653617\n",
      "Epoch: 289, Loss: 1.863817\n",
      "Epoch: 290, Loss: 1.072395\n",
      "Epoch: 291, Loss: 1.317519\n",
      "Epoch: 292, Loss: 1.249313\n",
      "Epoch: 293, Loss: 1.250967\n",
      "Epoch: 294, Loss: 1.150312\n",
      "Epoch: 295, Loss: 1.073376\n",
      "Epoch: 296, Loss: 1.139743\n",
      "Epoch: 297, Loss: 1.501464\n",
      "Epoch: 298, Loss: 1.289799\n",
      "Epoch: 299, Loss: 1.058628\n",
      "3473.059977054596\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 300\n",
    "l2_lambda = .001\n",
    "training(model, optimizer, loss_function, n_epochs, device, train_loader, l2_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8b348607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational complexity:       0.02 GMac\n",
      "Number of parameters:           272.68 k\n"
     ]
    }
   ],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "\n",
    "macs, params = get_model_complexity_info(model, ( 3, 32,32), as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "96afbcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  0.3615\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(val_transformed_cifar10, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size=imgs.shape[0]\n",
    "        outputs = model(imgs)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "#         print(predicted)\n",
    "#         print(\"\\n\")\n",
    "#         print(labels)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted==labels).sum())\n",
    "    print(\"Accuracy \", correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b0ccef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dbfaee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
