{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d3da6b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    " \n",
    "# Data Visualisation \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "df = pd.DataFrame(pd.read_csv(\"Housing.csv\")) \n",
    "df.replace(('yes', 'no'), (1, 0), inplace=True)\n",
    "df.replace(('furnished', 'semi-furnished', 'unfurnished'), (1, .5, 0), inplace=True)\n",
    "housing_N=(df-df.min())/(df.max()-df.min())\n",
    "housing_N = housing_N[['price', 'area', 'bedrooms', 'bathrooms', 'stories', 'parking']]\n",
    "#print(housing_N.head())\n",
    "\n",
    "housing_N = housing_N.astype('float32')\n",
    "\n",
    "t_un = housing_N.drop('price', axis=1, )\n",
    "print(type(t_un))\n",
    "t_c = housing_N['price']\n",
    "t_un = torch.from_numpy(t_un.to_numpy())\n",
    "t_c = torch.from_numpy(t_c.to_numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "912e326a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([545, 5])\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(t_un.shape)\n",
    "n_samples = t_un.shape[0]\n",
    "n_val = int(.2 * n_samples)\n",
    "shuffled_indices = torch.randperm(n_samples).numpy()\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "print(type(train_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "78894e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([436, 5])\n"
     ]
    }
   ],
   "source": [
    "train_t_un = t_un[train_indices]\n",
    "train_t_c = t_c[train_indices]\n",
    "\n",
    "val_t_un = t_un[val_indices]\n",
    "val_t_c = t_c[val_indices]\n",
    "print(train_t_un.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0ab012d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=5, out_features=8, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "seq_model = nn.Sequential(\n",
    "            nn.Linear(5, 8),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8, 1))\n",
    "seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1df7e015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
    "optimizer = optim.SGD(seq_model.parameters(), lr=1e-2)\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, t_un_train, t_un_val, t_c_train, t_c_val):\n",
    "    t_c_val = torch.unsqueeze(t_c_val, dim=-1)\n",
    "    t_c_train = torch.unsqueeze(t_c_train, dim=-1)\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        t_p_train = model(t_un_train)\n",
    "        #print(t_c_train.shape)\n",
    "        loss_train = loss_fn(t_p_train, t_c_train)\n",
    "        t_p_val = model(t_un_val)\n",
    "       \n",
    "        \n",
    "        loss_val = loss_fn(t_p_val, t_c_val)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        if epoch <  5 or epoch == 200:\n",
    "            print(f\"Epoch {epoch}, Training Loss {loss_train.item():.4f},\"f\" Validation loss {loss_val.item():.4f}\")\n",
    "     \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8fbf77b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Epoch 1, Training Loss 0.0164, Validation loss 0.0147\n",
      "Epoch 2, Training Loss 0.0164, Validation loss 0.0147\n",
      "Epoch 3, Training Loss 0.0164, Validation loss 0.0146\n",
      "Epoch 4, Training Loss 0.0163, Validation loss 0.0146\n",
      "Epoch 200, Training Loss 0.0149, Validation loss 0.0135\n",
      "0.16614627838134766\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seq_model.to(device)\n",
    "train_t_un = train_t_un.to(device)\n",
    "val_t_un = val_t_un.to(device)\n",
    "train_t_c = train_t_c.to(device)\n",
    "val_t_c = val_t_c.to(device)\n",
    "print(train_t_un.is_cuda)\n",
    "start = time.time()\n",
    "training_loop(n_epochs= 200, optimizer = optimizer, model = seq_model, loss_fn = nn.MSELoss(),\n",
    "t_un_train=train_t_un, t_un_val = val_t_un, t_c_train = train_t_c, t_c_val = val_t_c)\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "02f3a765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: variables __flops__ or __params__ are already defined for the moduleLinear ptflops can affect your code!\n",
      "Warning: variables __flops__ or __params__ are already defined for the moduleLinear ptflops can affect your code!\n",
      "Computational complexity:       0.0 GMac\n",
      "Number of parameters:           57      \n"
     ]
    }
   ],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "\n",
    "macs, params = get_model_complexity_info(seq_model, (436, 5), as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24d2bfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9565734267234802\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "   #seq_model.eval()\n",
    "    avg = 0\n",
    "    pred = []\n",
    "    for val in val_t_un:\n",
    "        pred.append(seq_model(val))\n",
    "    i = 0\n",
    "    for tar in val_t_c:\n",
    "        temp = pred[i]/tar\n",
    "        if temp > 1:\n",
    "            temp = 1 - (temp - 1)\n",
    "        if temp > 2:\n",
    "            temp = 1 - (temp - 2)\n",
    "        avg += abs(temp)\n",
    "        i += 1\n",
    "    ttl_avg = avg / len(val_t_un) \n",
    "print(ttl_avg[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d44feb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss 0.0434, Validation loss 0.0358\n",
      "Epoch 2, Training Loss 0.0434, Validation loss 0.0358\n",
      "Epoch 3, Training Loss 0.0434, Validation loss 0.0358\n",
      "Epoch 4, Training Loss 0.0434, Validation loss 0.0358\n",
      "Epoch 200, Training Loss 0.0434, Validation loss 0.0358\n",
      "0.2355363368988037\n"
     ]
    }
   ],
   "source": [
    "#PART B Model\n",
    "import time\n",
    "seq_model2 = nn.Sequential(\n",
    "            nn.Linear(5, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1))\n",
    "seq_model2.to(device)\n",
    "start = time.time()\n",
    "training_loop(n_epochs= 200, optimizer = optimizer, model = seq_model2, loss_fn = nn.MSELoss(),\n",
    "t_un_train=train_t_un, t_un_val = val_t_un, t_c_train = train_t_c, t_c_val = val_t_c)\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8df974c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational complexity:       0.02 GMac\n",
      "Number of parameters:           42.75 k \n"
     ]
    }
   ],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "\n",
    "macs, params = get_model_complexity_info(seq_model2, (436, 5), as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d8e65e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6140012145042419\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "   #seq_model.eval()\n",
    "    avg = 0\n",
    "    pred = []\n",
    "    for val in val_t_un:\n",
    "        pred.append(seq_model2(val))\n",
    "    i = 0\n",
    "    for tar in val_t_c:\n",
    "        temp = pred[i]/tar\n",
    "        if temp > 1:\n",
    "            temp = 1 - (temp - 1)\n",
    "        if temp > 2:\n",
    "            temp = 1 - (temp - 2)\n",
    "        avg += abs(temp)\n",
    "        i += 1\n",
    "    ttl_avg = avg / len(val_t_un) \n",
    "print(abs(ttl_avg[0].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70dbe8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# PART 2\n",
    "import ssl\n",
    "import torch\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "from torchvision import datasets\n",
    "data_path = \"./data/\"\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True)\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be6f843c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32, 10000])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "tensor_cifar10 = datasets.CIFAR10(data_path, train=True, download=False, transform=transforms.ToTensor())\n",
    "imgs = torch.stack([img_t for img_t, _ in tensor_cifar10], dim=3)\n",
    "imgs.shape\n",
    "tensor_cifar10_val = datasets.CIFAR10(data_path, train=False, download=False, transform=transforms.ToTensor())\n",
    "imgs_val = torch.stack([img_t for img_t, _ in tensor_cifar10_val], dim=3)\n",
    "imgs_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19759977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4942, 0.4851, 0.4504])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_val.view(3, -1).mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a256ede4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2467, 0.2429, 0.2616])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_val.view(3, -1).std(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c95cf84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4914, 0.4822, 0.4465])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.view(3, -1).mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f0f9527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2470, 0.2435, 0.2616])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.view(3, -1).std(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d623e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "transforms.Normalize((0.4914, 0.4822, 0.4465),  (0.2470, 0.2435, 0.2616))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c34b2c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "transformed_cifar10 = datasets.CIFAR10(data_path, train=True, download=False, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465),  (0.2470, 0.2435, 0.2616))]))\n",
    "img_t, _  = transformed_cifar10[99]\n",
    "plt.imshow(img_t.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "992a7670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalize(mean=(0.4942, 0.4851, 0.4504), std=(0.2467, 0.2429, 0.2616))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.Normalize((0.4942, 0.4851, 0.4504),  (0.2467, 0.2429, 0.2616))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c45f3319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUdklEQVR4nO3df5BeVX3H8feXkJjARhPIAitJWH7ECk0w0CUCKgpWCT86wbZQGbWpzRhawZKWdiYDVdBOK7YFizPIGJtorFFJFQrFaKWBAg4pZEFIghECNJrIkh9CJBEibPLtH8/NzBLv9+zm+XGfTc7nNZPZJ+e7594vl/3m7nPPc84xd0dEDnwHtTsBEamGil0kEyp2kUyo2EUyoWIXyYSKXSQTBzfS2cxmAjcBI4B/dffrU98/YcIE7+7ubuSUIpKwfv16tm7damWxuovdzEYANwPvAzYCK83sTnf/cdSnu7ub3t7eek8pIoPo6ekJY438Gj8DeNrdn3X3V4FvAbMaOJ6ItFAjxX40sGHA3zcWbSIyDDVS7GXvC37js7dmNtfMes2sd8uWLQ2cTkQa0UixbwQmDfj7ROC5vb/J3Re4e4+793R2djZwOhFpRCPFvhKYYmbHmtko4IPAnc1JS0Sare6n8e7eb2ZXAP9Fbehtkbs/0bTMRKSpGhpnd/dlwLIm5SIiLaRP0IlkQsUukgkVu0gmVOwimVCxi2SioafxB6KJVjphCIA/nzajtP2aVQ+1Kp2mefWO74axUbMuqDATaRfd2UUyoWIXyYSKXSQTKnaRTKjYRTKR5dP4L515Zhj7eaLf365+uLT9mgbzqcJVF10Yxm5P9Nuo7cEOGLqzi2RCxS6SCRW7SCZU7CKZULGLZELFLpKJLIfeblmxoqnHu+fmRWHsnMv/tKnnqtd/JmKp4cYvzf5oGLts8Vfqzkeqpzu7SCZU7CKZULGLZELFLpIJFbtIJlTsIploaOjNzNYD24FdQL+7xzvBDyNv7XhLGHt8x1P7fLy7738wjFU99PbpYA29n9Z5vDUry2f6yf6nGePsZ7v71iYcR0RaSL/Gi2Si0WJ34Adm9oiZzW1GQiLSGo3+Gv8Od3/OzI4A7jazn7j7/QO/ofhHYC7A5MmTGzydiNSroTu7uz9XfN1MbXWj39hFwd0XuHuPu/d0dnY2cjoRaUDdxW5mh5rZ2D2vgfcDa5qVmIg0VyO/xh8J3G61oZ6DgW+4+/ebklWLfWv7k2Hs1sT2T5Hrly4MY5+95PfC2Asb4gGxa//yqjDWR38YiyP1+Zu/+4cmH1Hape5id/dngbc1MRcRaSENvYlkQsUukgkVu0gmVOwimVCxi2QiywUnqzTiDy8KY6MT/V6u83y/E7TPT/T5xLuOC2Nv/oNZdWYiw43u7CKZULGLZELFLpIJFbtIJlTsIpk4cJ/Gz58Xhh5eeFNlaZyXiH0sETsjERuXiI0K/o++nJghs+OBZ+Pgki/GsQ99PJGJDDe6s4tkQsUukgkVu0gmVOwimVCxi2RCxS6Sif1j6G3JF0qbez58ZdjlkcThDq8zjT8K2lPDZBMTsbMTsdQkmZ2pWDDEltqyZ30qjw9fHsbOXPbtuOOSexJHlXbQnV0kEyp2kUyo2EUyoWIXyYSKXSQTKnaRTAw69GZmi4ALgc3uPrVoOwy4FeimNnJzibu/2FAmc+JtkkYsuqu0fXficMcnYs8kYqMSsVOC9m2JPs8nYr2J2LhEbGoiNikRi3QnYkclYi99494w1vuN8m20JpwdDziefI+G61ppKHf2rwIz92qbDyx39ynActLrGYrIMDBosRf7rb+wV/MsYHHxejFwUXPTEpFmq/c9+5Hu3gdQfD2ieSmJSCu0/AGdmc01s14z692yZUurTycigXqLfZOZdQEUXzdH3+juC9y9x917Ojs76zydiDSq3mK/E5hdvJ4N3NGcdESkVYYy9PZN4D3ABDPbCFwLXA8sNbM5wM+Ai4d0tg3r2T3vo6WhaHitXh9JxD6TiL2aiF0dtKeGAFvh7YlYtIhlNGwI6WG+Q45N/Ih0xHPzpq7eUdr+T/fGw3WXXh8vbnnqfC1u2ahBi93dLw1C721yLiLSQvoEnUgmVOwimVCxi2RCxS6SCRW7SCbM3Ss72cFmPi6I/aKO481LxN6aiP1ZHefaX8wJ2usdepuWiJ08MTGYE618eX5i97vFyxJnk6Ho6emht7e3dMqh7uwimVCxi2RCxS6SCRW7SCZU7CKZULGLZKLSvd52Ud8Q25FBe2oftY11nKcVDknEXq7zmL+ViHUF7R2JPsEgGQCrU/02xj2nBhPiRo0+NHFEaSXd2UUyoWIXyYSKXSQTKnaRTKjYRTJR6dP4en0iaF9Z5/HemIiNScQ21XGu1BP31Lpe8epu6YkrE4L21LZQqS2eUlJP8bfuLG9/823/EXc68wtxbPZfDCUlSdCdXSQTKnaRTKjYRTKhYhfJhIpdJBMqdpFMDGX7p0XAhcBmd59atF1HbaehPduyXu3uDS0gltrSKBqG6k30SU38GJmIpdauezFoT20ZlbI8EXt3IpYaRqtnAlDqWk0fF8f6Ez89D2wtb//81njAru9Prgxjx356SRj75OMPhbFRY8NQdoZyZ/8qMLOk/fPuPr34o5UCRYa5QYvd3e8HXqggFxFpoUbes19hZqvMbJGZjW9aRiLSEvUW+y3A8cB0oA+4IfpGM5trZr1mlnqLLSItVlexu/smd9/l7ruBLwMzEt+7wN173L2n3iRFpHF1FbuZDVz96APAmuakIyKtMuj2T2b2TeA91CZUbQKuLf4+HXBgPXCZu/cNejKz8GSpLZmiobcv1tEH4KVE7PhE7ISgPfX+pJ419xpxbtA+pc7jpdagu6/OY1Zp/pWfKW3/7L98suJMqpHa/mnQcXZ3v7SkeWHDWYlIpfQJOpFMqNhFMqFiF8mEil0kEyp2kUxUuuDkG4i3bPq/RL8dQXtqtlm9M9E2JGLRYo7vSfRJDcv9dNBs9t36oP3eRJ96r1XK4UH7uESfZxKxYxKxaEgUYNtNnyptf/i8C8M+M849JXHE/Zfu7CKZULGLZELFLpIJFbtIJlTsIplQsYtkotKht9HECzpGw2uQ3lOs2VLDUE8H7amLmFrAshVDb0+24Jj1iIYptyX6XJCYq3gsweZxg5g6rnw5zRPuXRx32g+G3p66Z1Fp+87twUqf6M4ukg0Vu0gmVOwimVCxi2RCxS6SiUHXoGumDrPa/lH7KHpGm1oDLTVxohVPwWX/kvr5uHTKSWHss/fdFHfsSqw4+EoiNib6CT8y7sOq0taenpvp7d1Yugad7uwimVCxi2RCxS6SCRW7SCZU7CKZULGLZGLQiTBmNgn4GnAUsBtY4O43mdlhwK1AN7Wlzy5x9xdTx+onnggxLtGvnikQ0UQM0NCbpH8Grl/34zC2YvL7wtjSb8cDeitXxmecMu1Npe0Tu44O+xxyVDAstzPe3Gwod/Z+4Cp3PxE4HbjczE4C5gPL3X0KsLz4u4gMU4MWu7v3ufujxevtwFrgaGAWsGee4GLgohblKCJNsE/v2c2sGzgFeAg4cs/OrcXXI5qenYg0zZAXrzCzDuA7wDx3f8ms9BN5Zf3mAnP36WQi0nRDurOb2Uhqhb7E3W8rmjeZWVcR7wI2l/V19wXu3uPuPSOakbGI1GXQYrfaLXwhsNbdbxwQuhOYXbyeDdzR/PREpFkGnfVmZu8EHgBWUxt6A7ia2vv2pcBk4GfAxe7+QupYbzLzM4JYap2554P21JZRnYnYcBl6S/1Le+P58Yynecs2NT8ZGZJzp8WxY0+LYydMKR9eA1i79pel7TsSY879wUS55d+FF7d66XvsQd9Gu/sPgegN+nsH6y8iw4M+QSeSCRW7SCZU7CKZULGLZELFLpKJShecHG/m0eP7eNMaWBO0p6bY7U7EDmTRvKvUtYrnSeXp8ETsueXnhbGvLPteGDttWjwjbsWD5YPBaxJFsWJDeftTT8DLvyofetOdXSQTKnaRTKjYRTKhYhfJhIpdJBMqdpFMVDr09kYznxHEopltAOuC9lcbzKfdUjt5Lbsyjp768XjRQ57fVtr84K3/HXY5fcabw9hBZ5wcxh5e8P0w9vYb6lkmdHi474/j+WFnLb4s7pjaz210Ys7Ztp+Xt3ccGvd5vvxcPResonfVDg29ieRMxS6SCRW7SCZU7CKZULGLZKLS1Z1HEG/ztD7RL7U+3f7s+buisQngglPi2FNPxbEp5f9Lz5xzXNyn6y2JWPw0fsa18ZSRr29YWNr+4aXxqap038eDRdyAs27+q0TPxBP3Manxlfh8jA6exvfvivtMCo43Kr5/684ukgkVu0gmVOwimVCxi2RCxS6SCRW7SCYGHXozs0nA14CjqC3ttsDdbzKz64CPAVuKb73a3ZeljrWN2u6QOXl3Ktg1ORFMDOOM/3Uc2xIM4/QlJqZ0JNLoSpxr7Lgw9KGvX1XeftkzYZ8H738ujG3ZEveb9f6TwhizojXjEsONpLbXSq1QNyERSxgTLTaX+O/aHkxs2h3vrjyUcfZ+4Cp3f9TMxgKPmNndRezz7v7PQziGiLTZUPZ66wP6itfbzWwtcHSrExOR5tqn9+xm1g2cQm0HV4ArzGyVmS0ys/HNTk5EmmfIxW5mHdTecs9z95eAW4DjgenU7vw3BP3mmlmvmfU2nq6I1GtIxW5mI6kV+hJ3vw3A3Te5+y533w18GSj9oLe7L3D3HnfvaVbSIrLvBi12MzNgIbDW3W8c0N414Ns+QLxxi4gMA0N5Gv8O4CPAajN7rGi7GrjUzKYDTm3SWmJxrhoDRgax/X09ucgZqeC6lXFsWmLobUMwvAbhemabVz8bdtmZiE2ePSc+V9epcWzkm8rbz4nXzzvznCPi4yX9OBGLZo4lhrUIck8eD2BaIvZAIhY9705c37E/Km8/KC7poTyN/yG1Ot1bckxdRIYXfYJOJBMqdpFMqNhFMqFiF8mEil0kE5UuOGmJEx6oQ28bErG//9xPw9g1G26OO45PTFObWD581dEZ97l92Y4w9hHKF44EOOT8xMKXnceXtx+VmG2285dxbOuqOHZwaknSYLbf6H+Pu4z/7Ti24cE4NumDiTwSC06GS6omhlg5O2iPh/h0ZxfJhIpdJBMqdpFMqNhFMqFiF8mEil0kE5UOve0GXq7yhBVJDNQw6cQ4NqErjjEmETvltDg2rny23CFT4gUnPzYumEEFHNSfGtZK/Pj0PVne/mJiMceuxEy/1xILZr6WyHFb+YKZL/3orrDLG09cHR9vQmLPvFe+F8fGxLP94GdBe2qGXbRI5a/CHrqzi2RCxS6SCRW7SCZU7CKZULGLZELFLpKJSofeDlQnJGLjpsSxoxIxTnxXHJuUmDkWTa7qjxdRPGhG4lypobLxiYUZJ50cBFJDaL9InCuVY2LByfXls+W2ToiH+R5bHc9GPOv3Z8bn2pGYpTYm3seutu9Kiadujbt0Hlrevivem093dpFMqNhFMqFiF8mEil0kEyp2kUwM+jTezEYD9wNvKL7/2+5+rZkdBtwKdFPb/ukSd3+xdam239uD9o2JJeEeeDSOdSeexu9+JY4d1J/43zb2mPL2V+InzGzZFsfC9dEgNekCj55MJ3Lflnga35E41/jEBJpp5aMCox+Nn+Cfflo0kgAcnMi/I7j2AK8EE4MAxgTXKjVRakswqtG/O+wylDv7r4Fz3P1t1LZnnmlmpwPzgeXuPgVYXvxdRIapQYvda/YsPzqy+OPALGBx0b4YuKgVCYpIcwx1f/YRxQ6um4G73f0h4Eh37wMovta7BaeIVGBIxe7uu9x9OjARmGFmU4d6AjOba2a9ZtZbZ44i0gT79DTe3bcB/wPMBDaZWRdA8XVz0GeBu/e4e09jqYpIIwYtdjPrNLNxxesxwO8CPwHuBGYX3zYbuKNFOYpIEwxlIkwXsNjMRlD7x2Gpu99lZiuApWY2h9oiWhe3MM9h4eJgPbkTzoj7/O+6OLYztXTatngCymFrExM/+oJhnP7E1kr9ickuiWFFEsODrAuOOSbxI/da4ngdqUdCwaQQgAnlF3nrzvgarl4Rx86NJqAA9Ce2eEptbUWwjVY4mQjYFvQxD7sMWuzuvoqSaTnu/gvgvYP1F5HhQZ+gE8mEil0kEyp2kUyo2EUyoWIXyYS5x4/qm34ysy3AnulXE4j3sKmS8ng95fF6+1sex7h7Z1mg0mJ/3YnNeofDp+qUh/LIJQ/9Gi+SCRW7SCbaWewL2njugZTH6ymP1ztg8mjbe3YRqZZ+jRfJRFuK3cxmmtmTZva0mbVt7TozW29mq83ssSoX1zCzRWa22czWDGg7zMzuNrN1xdfxbcrjOjP7eXFNHjOz8yvIY5KZ3Wtma83sCTO7smiv9Jok8qj0mpjZaDN72MweL/L4dNHe2PVw90r/ACOAZ4DjgFHA48BJVedR5LIemNCG854FnAqsGdD2j8D84vV84HNtyuM64K8rvh5dwKnF67HU5nyeVPU1SeRR6TUBDOgoXo8EHgJOb/R6tOPOPgN42t2fdfdXgW9RW7wyG+5+P/DCXs2VL+AZ5FE5d+9z90eL19uBtcDRVHxNEnlUymuavshrO4r9aGDDgL9vpA0XtODAD8zsETOb26Yc9hhOC3heYWaril/zW/52YiAz66a2fkJbFzXdKw+o+Jq0YpHXdhS7lbS1a0jgHe5+KnAecLmZndWmPIaTW4Djqe0R0AfcUNWJzawD+A4wz91fquq8Q8ij8mviDSzyGmlHsW8EJg34+0QgtXl1y7j7c8XXzcDt1N5itMuQFvBsNXffVPyg7Qa+TEXXxMxGUiuwJe5+W9Fc+TUpy6Nd16Q49zb2cZHXSDuKfSUwxcyONbNRwAepLV5ZKTM71MzG7nkNvB9Yk+7VUsNiAc89P0yFD1DBNTEzAxYCa939xgGhSq9JlEfV16Rli7xW9YRxr6eN51N70vkMcE2bcjiO2kjA48ATVeYBfJPar4OvUftNZw5wOLVttNYVXw9rUx7/BqwGVhU/XF0V5PFOam/lVgGPFX/Or/qaJPKo9JoAJwM/Ks63BvhU0d7Q9dAn6EQyoU/QiWRCxS6SCRW7SCZU7CKZULGLZELFLpIJFbtIJlTsIpn4f2dnCGPV3iOBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_transformed_cifar10 = datasets.CIFAR10(data_path, train=False, download=False, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465),  (0.2470, 0.2435, 0.2616))]))\n",
    "img_t, _  = val_transformed_cifar10[99]\n",
    "plt.imshow(img_t.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3358f078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Epoch: 0, Loss: 2.114938\n",
      "Epoch: 1, Loss: 1.615053\n",
      "Epoch: 2, Loss: 1.872099\n",
      "Epoch: 3, Loss: 1.396578\n",
      "Epoch: 4, Loss: 1.444824\n",
      "Epoch: 5, Loss: 1.222077\n",
      "Epoch: 6, Loss: 1.918046\n",
      "Epoch: 7, Loss: 1.559697\n",
      "Epoch: 8, Loss: 1.983501\n",
      "Epoch: 9, Loss: 0.949044\n",
      "Epoch: 10, Loss: 1.209287\n",
      "Epoch: 11, Loss: 1.450662\n",
      "Epoch: 12, Loss: 0.970523\n",
      "Epoch: 13, Loss: 1.285719\n",
      "Epoch: 14, Loss: 1.732461\n",
      "Epoch: 15, Loss: 1.192126\n",
      "Epoch: 16, Loss: 0.910734\n",
      "Epoch: 17, Loss: 1.393086\n",
      "Epoch: 18, Loss: 1.241186\n",
      "Epoch: 19, Loss: 1.125187\n",
      "Epoch: 20, Loss: 0.954129\n",
      "Epoch: 21, Loss: 0.989331\n",
      "Epoch: 22, Loss: 0.913009\n",
      "Epoch: 23, Loss: 1.136506\n",
      "Epoch: 24, Loss: 1.223707\n",
      "Epoch: 25, Loss: 0.985604\n",
      "Epoch: 26, Loss: 1.219096\n",
      "Epoch: 27, Loss: 1.048434\n",
      "Epoch: 28, Loss: 1.004331\n",
      "Epoch: 29, Loss: 0.470501\n",
      "Epoch: 30, Loss: 0.640969\n",
      "Epoch: 31, Loss: 0.751816\n",
      "Epoch: 32, Loss: 1.326371\n",
      "Epoch: 33, Loss: 0.759471\n",
      "Epoch: 34, Loss: 0.568612\n",
      "Epoch: 35, Loss: 0.983330\n",
      "Epoch: 36, Loss: 0.340566\n",
      "Epoch: 37, Loss: 0.565999\n",
      "Epoch: 38, Loss: 0.317416\n",
      "Epoch: 39, Loss: 0.478743\n",
      "Epoch: 40, Loss: 0.383954\n",
      "Epoch: 41, Loss: 0.194871\n",
      "Epoch: 42, Loss: 0.383883\n",
      "Epoch: 43, Loss: 0.356524\n",
      "Epoch: 44, Loss: 0.532261\n",
      "Epoch: 45, Loss: 0.296914\n",
      "Epoch: 46, Loss: 0.301609\n",
      "Epoch: 47, Loss: 0.631322\n",
      "Epoch: 48, Loss: 0.722061\n",
      "Epoch: 49, Loss: 0.323737\n",
      "Epoch: 50, Loss: 0.216566\n",
      "Epoch: 51, Loss: 0.242049\n",
      "Epoch: 52, Loss: 0.221476\n",
      "Epoch: 53, Loss: 0.262617\n",
      "Epoch: 54, Loss: 0.258945\n",
      "Epoch: 55, Loss: 0.193806\n",
      "Epoch: 56, Loss: 0.366305\n",
      "Epoch: 57, Loss: 0.053945\n",
      "Epoch: 58, Loss: 0.563612\n",
      "Epoch: 59, Loss: 0.109790\n",
      "Epoch: 60, Loss: 0.417546\n",
      "Epoch: 61, Loss: 0.130372\n",
      "Epoch: 62, Loss: 0.175635\n",
      "Epoch: 63, Loss: 0.168109\n",
      "Epoch: 64, Loss: 0.113282\n",
      "Epoch: 65, Loss: 0.162943\n",
      "Epoch: 66, Loss: 0.116102\n",
      "Epoch: 67, Loss: 0.131673\n",
      "Epoch: 68, Loss: 0.055625\n",
      "Epoch: 69, Loss: 0.305790\n",
      "Epoch: 70, Loss: 0.145838\n",
      "Epoch: 71, Loss: 0.137139\n",
      "Epoch: 72, Loss: 0.101728\n",
      "Epoch: 73, Loss: 0.280011\n",
      "Epoch: 74, Loss: 0.074860\n",
      "Epoch: 75, Loss: 0.066992\n",
      "Epoch: 76, Loss: 0.104298\n",
      "Epoch: 77, Loss: 0.046545\n",
      "Epoch: 78, Loss: 0.059150\n",
      "Epoch: 79, Loss: 0.063262\n",
      "Epoch: 80, Loss: 0.083582\n",
      "Epoch: 81, Loss: 0.057308\n",
      "Epoch: 82, Loss: 0.136771\n",
      "Epoch: 83, Loss: 0.075094\n",
      "Epoch: 84, Loss: 0.031949\n",
      "Epoch: 85, Loss: 0.070973\n",
      "Epoch: 86, Loss: 0.056512\n",
      "Epoch: 87, Loss: 0.052779\n",
      "Epoch: 88, Loss: 0.060463\n",
      "Epoch: 89, Loss: 0.080794\n",
      "Epoch: 90, Loss: 0.065539\n",
      "Epoch: 91, Loss: 0.032500\n",
      "Epoch: 92, Loss: 0.059449\n",
      "Epoch: 93, Loss: 0.054496\n",
      "Epoch: 94, Loss: 0.057564\n",
      "Epoch: 95, Loss: 0.035047\n",
      "Epoch: 96, Loss: 0.027680\n",
      "Epoch: 97, Loss: 0.041070\n",
      "Epoch: 98, Loss: 0.062549\n",
      "Epoch: 99, Loss: 0.060719\n",
      "Epoch: 100, Loss: 0.034918\n",
      "Epoch: 101, Loss: 0.081994\n",
      "Epoch: 102, Loss: 0.074913\n",
      "Epoch: 103, Loss: 0.035570\n",
      "Epoch: 104, Loss: 0.050432\n",
      "Epoch: 105, Loss: 0.075468\n",
      "Epoch: 106, Loss: 0.049921\n",
      "Epoch: 107, Loss: 0.048725\n",
      "Epoch: 108, Loss: 0.044439\n",
      "Epoch: 109, Loss: 0.026217\n",
      "Epoch: 110, Loss: 0.090040\n",
      "Epoch: 111, Loss: 0.036318\n",
      "Epoch: 112, Loss: 0.041751\n",
      "Epoch: 113, Loss: 0.018165\n",
      "Epoch: 114, Loss: 0.023618\n",
      "Epoch: 115, Loss: 0.030256\n",
      "Epoch: 116, Loss: 0.023533\n",
      "Epoch: 117, Loss: 0.033315\n",
      "Epoch: 118, Loss: 0.020244\n",
      "Epoch: 119, Loss: 0.030456\n",
      "Epoch: 120, Loss: 0.038001\n",
      "Epoch: 121, Loss: 0.021742\n",
      "Epoch: 122, Loss: 0.042539\n",
      "Epoch: 123, Loss: 0.044400\n",
      "Epoch: 124, Loss: 0.019957\n",
      "Epoch: 125, Loss: 0.015826\n",
      "Epoch: 126, Loss: 0.028996\n",
      "Epoch: 127, Loss: 0.024071\n",
      "Epoch: 128, Loss: 0.034205\n",
      "Epoch: 129, Loss: 0.026615\n",
      "Epoch: 130, Loss: 0.024524\n",
      "Epoch: 131, Loss: 0.025195\n",
      "Epoch: 132, Loss: 0.018350\n",
      "Epoch: 133, Loss: 0.009029\n",
      "Epoch: 134, Loss: 0.022213\n",
      "Epoch: 135, Loss: 0.012650\n",
      "Epoch: 136, Loss: 0.020019\n",
      "Epoch: 137, Loss: 0.035109\n",
      "Epoch: 138, Loss: 0.047220\n",
      "Epoch: 139, Loss: 0.018229\n",
      "Epoch: 140, Loss: 0.009324\n",
      "Epoch: 141, Loss: 0.037086\n",
      "Epoch: 142, Loss: 0.018808\n",
      "Epoch: 143, Loss: 0.018705\n",
      "Epoch: 144, Loss: 0.015033\n",
      "Epoch: 145, Loss: 0.034881\n",
      "Epoch: 146, Loss: 0.018375\n",
      "Epoch: 147, Loss: 0.023852\n",
      "Epoch: 148, Loss: 0.020016\n",
      "Epoch: 149, Loss: 0.015935\n",
      "Epoch: 150, Loss: 0.026944\n",
      "Epoch: 151, Loss: 0.016342\n",
      "Epoch: 152, Loss: 0.015106\n",
      "Epoch: 153, Loss: 0.012081\n",
      "Epoch: 154, Loss: 0.032647\n",
      "Epoch: 155, Loss: 0.021861\n",
      "Epoch: 156, Loss: 0.013086\n",
      "Epoch: 157, Loss: 0.021615\n",
      "Epoch: 158, Loss: 0.023154\n",
      "Epoch: 159, Loss: 0.012029\n",
      "Epoch: 160, Loss: 0.021657\n",
      "Epoch: 161, Loss: 0.016932\n",
      "Epoch: 162, Loss: 0.024478\n",
      "Epoch: 163, Loss: 0.010082\n",
      "Epoch: 164, Loss: 0.015802\n",
      "Epoch: 165, Loss: 0.022901\n",
      "Epoch: 166, Loss: 0.011196\n",
      "Epoch: 167, Loss: 0.018111\n",
      "Epoch: 168, Loss: 0.014098\n",
      "Epoch: 169, Loss: 0.016492\n",
      "Epoch: 170, Loss: 0.016197\n",
      "Epoch: 171, Loss: 0.007443\n",
      "Epoch: 172, Loss: 0.019747\n",
      "Epoch: 173, Loss: 0.019054\n",
      "Epoch: 174, Loss: 0.015939\n",
      "Epoch: 175, Loss: 0.024195\n",
      "Epoch: 176, Loss: 0.010328\n",
      "Epoch: 177, Loss: 0.010470\n",
      "Epoch: 178, Loss: 0.016593\n",
      "Epoch: 179, Loss: 0.011979\n",
      "Epoch: 180, Loss: 0.018716\n",
      "Epoch: 181, Loss: 0.009796\n",
      "Epoch: 182, Loss: 0.011112\n",
      "Epoch: 183, Loss: 0.021291\n",
      "Epoch: 184, Loss: 0.022117\n",
      "Epoch: 185, Loss: 0.007476\n",
      "Epoch: 186, Loss: 0.017741\n",
      "Epoch: 187, Loss: 0.013741\n",
      "Epoch: 188, Loss: 0.012636\n",
      "Epoch: 189, Loss: 0.007945\n",
      "Epoch: 190, Loss: 0.007332\n",
      "Epoch: 191, Loss: 0.014015\n",
      "Epoch: 192, Loss: 0.012144\n",
      "Epoch: 193, Loss: 0.015187\n",
      "Epoch: 194, Loss: 0.009774\n",
      "Epoch: 195, Loss: 0.011758\n",
      "Epoch: 196, Loss: 0.009519\n",
      "Epoch: 197, Loss: 0.007883\n",
      "Epoch: 198, Loss: 0.014944\n",
      "Epoch: 199, Loss: 0.013657\n",
      "1840.6029827594757\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "print(torch.cuda.is_available())\n",
    "import time\n",
    "device = torch.device('cuda:0')\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "train_loader = torch.utils.data.DataLoader(transformed_cifar10, batch_size=64, shuffle=True, **kwargs)\n",
    "model = nn.Sequential(nn.Linear(3072, 512), nn.Tanh(), nn.Linear(512, 10), nn.LogSoftmax(dim=1))\n",
    "model.to(device)\n",
    "#transformed_cifar10.to(device)\n",
    "loss_fn = nn.NLLLoss()\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "n_epochs = 200\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1).to(device))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed474ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: variables __flops__ or __params__ are already defined for the moduleLinear ptflops can affect your code!\n",
      "Warning: variables __flops__ or __params__ are already defined for the moduleLinear ptflops can affect your code!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3072x1 and 3072x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_40192/2773317352.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmacs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_model_complexity_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_strings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_per_layer_stat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{:<30}  {:<8}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Computational complexity: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmacs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{:<30}  {:<8}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Number of parameters: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Standard Python\\lib\\site-packages\\ptflops\\flops_counter.py\u001b[0m in \u001b[0;36mget_model_complexity_info\u001b[1;34m(model, input_res, print_per_layer_stat, as_strings, input_constructor, ost, verbose, ignore_modules, custom_modules_hooks)\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_empty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minput_res\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflops_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mflops_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflops_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_average_flops_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Standard Python\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1118\u001b[0m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1120\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Standard Python\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Standard Python\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1118\u001b[0m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1120\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Standard Python\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Standard Python\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3072x1 and 3072x512)"
     ]
    }
   ],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "\n",
    "macs, params = get_model_complexity_info(model, (3, 32, 32, 1), as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43616601",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(val_transformed_cifar10, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size=imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted==labels).sum())\n",
    "    print(\"Accuracy %f\", correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97b9e750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Epoch: 0, Loss: 1.905331\n",
      "Epoch: 1, Loss: 1.514360\n",
      "Epoch: 2, Loss: 2.005430\n",
      "Epoch: 3, Loss: 1.703515\n",
      "Epoch: 4, Loss: 1.799549\n",
      "Epoch: 5, Loss: 1.650682\n",
      "Epoch: 6, Loss: 1.539779\n",
      "Epoch: 7, Loss: 1.950585\n",
      "Epoch: 8, Loss: 1.522091\n",
      "Epoch: 9, Loss: 1.372893\n",
      "Epoch: 10, Loss: 1.200233\n",
      "Epoch: 11, Loss: 1.695626\n",
      "Epoch: 12, Loss: 1.362739\n",
      "Epoch: 13, Loss: 1.260988\n",
      "Epoch: 14, Loss: 1.443078\n",
      "Epoch: 15, Loss: 0.808057\n",
      "Epoch: 16, Loss: 1.433313\n",
      "Epoch: 17, Loss: 1.625645\n",
      "Epoch: 18, Loss: 0.945271\n",
      "Epoch: 19, Loss: 1.018353\n",
      "Epoch: 20, Loss: 0.541330\n",
      "Epoch: 21, Loss: 1.419379\n",
      "Epoch: 22, Loss: 0.533874\n",
      "Epoch: 23, Loss: 0.926448\n",
      "Epoch: 24, Loss: 1.037093\n",
      "Epoch: 25, Loss: 1.285270\n",
      "Epoch: 26, Loss: 0.516713\n",
      "Epoch: 27, Loss: 0.530516\n",
      "Epoch: 28, Loss: 0.918009\n",
      "Epoch: 29, Loss: 0.953247\n",
      "Epoch: 30, Loss: 0.402326\n",
      "Epoch: 31, Loss: 0.659886\n",
      "Epoch: 32, Loss: 0.487885\n",
      "Epoch: 33, Loss: 0.377578\n",
      "Epoch: 34, Loss: 0.429737\n",
      "Epoch: 35, Loss: 0.333303\n",
      "Epoch: 36, Loss: 0.626463\n",
      "Epoch: 37, Loss: 0.649741\n",
      "Epoch: 38, Loss: 0.251168\n",
      "Epoch: 39, Loss: 0.274291\n",
      "Epoch: 40, Loss: 0.194813\n",
      "Epoch: 41, Loss: 0.207591\n",
      "Epoch: 42, Loss: 0.409367\n",
      "Epoch: 43, Loss: 0.188203\n",
      "Epoch: 44, Loss: 0.045916\n",
      "Epoch: 45, Loss: 0.305902\n",
      "Epoch: 46, Loss: 0.178774\n",
      "Epoch: 47, Loss: 0.125839\n",
      "Epoch: 48, Loss: 0.308249\n",
      "Epoch: 49, Loss: 0.067167\n",
      "Epoch: 50, Loss: 0.040886\n",
      "Epoch: 51, Loss: 0.045552\n",
      "Epoch: 52, Loss: 0.279529\n",
      "Epoch: 53, Loss: 0.024319\n",
      "Epoch: 54, Loss: 0.064985\n",
      "Epoch: 55, Loss: 0.022207\n",
      "Epoch: 56, Loss: 0.028649\n",
      "Epoch: 57, Loss: 0.009787\n",
      "Epoch: 58, Loss: 0.048132\n",
      "Epoch: 59, Loss: 0.011765\n",
      "Epoch: 60, Loss: 0.009761\n",
      "Epoch: 61, Loss: 0.072844\n",
      "Epoch: 62, Loss: 0.052341\n",
      "Epoch: 63, Loss: 0.005202\n",
      "Epoch: 64, Loss: 0.013462\n",
      "Epoch: 65, Loss: 0.243351\n",
      "Epoch: 66, Loss: 0.046177\n",
      "Epoch: 67, Loss: 0.002374\n",
      "Epoch: 68, Loss: 0.012922\n",
      "Epoch: 69, Loss: 0.008812\n",
      "Epoch: 70, Loss: 0.002817\n",
      "Epoch: 71, Loss: 0.003548\n",
      "Epoch: 72, Loss: 0.004478\n",
      "Epoch: 73, Loss: 0.002186\n",
      "Epoch: 74, Loss: 0.003435\n",
      "Epoch: 75, Loss: 0.004121\n",
      "Epoch: 76, Loss: 0.005953\n",
      "Epoch: 77, Loss: 0.004203\n",
      "Epoch: 78, Loss: 0.003902\n",
      "Epoch: 79, Loss: 0.003709\n",
      "Epoch: 80, Loss: 0.004422\n",
      "Epoch: 81, Loss: 0.002876\n",
      "Epoch: 82, Loss: 0.001638\n",
      "Epoch: 83, Loss: 0.002497\n",
      "Epoch: 84, Loss: 0.001455\n",
      "Epoch: 85, Loss: 0.001799\n",
      "Epoch: 86, Loss: 0.004352\n",
      "Epoch: 87, Loss: 0.003661\n",
      "Epoch: 88, Loss: 0.003072\n",
      "Epoch: 89, Loss: 0.001326\n",
      "Epoch: 90, Loss: 0.001735\n",
      "Epoch: 91, Loss: 0.002697\n",
      "Epoch: 92, Loss: 0.001692\n",
      "Epoch: 93, Loss: 0.001145\n",
      "Epoch: 94, Loss: 0.002506\n",
      "Epoch: 95, Loss: 0.001077\n",
      "Epoch: 96, Loss: 0.001444\n",
      "Epoch: 97, Loss: 0.004064\n",
      "Epoch: 98, Loss: 0.001627\n",
      "Epoch: 99, Loss: 0.001321\n",
      "Epoch: 100, Loss: 0.003165\n",
      "Epoch: 101, Loss: 0.002776\n",
      "Epoch: 102, Loss: 0.001906\n",
      "Epoch: 103, Loss: 0.001515\n",
      "Epoch: 104, Loss: 0.001948\n",
      "Epoch: 105, Loss: 0.002734\n",
      "Epoch: 106, Loss: 0.002137\n",
      "Epoch: 107, Loss: 0.001067\n",
      "Epoch: 108, Loss: 0.002072\n",
      "Epoch: 109, Loss: 0.000689\n",
      "Epoch: 110, Loss: 0.001886\n",
      "Epoch: 111, Loss: 0.001659\n",
      "Epoch: 112, Loss: 0.001441\n",
      "Epoch: 113, Loss: 0.001860\n",
      "Epoch: 114, Loss: 0.001144\n",
      "Epoch: 115, Loss: 0.002507\n",
      "Epoch: 116, Loss: 0.001729\n",
      "Epoch: 117, Loss: 0.001596\n",
      "Epoch: 118, Loss: 0.000437\n",
      "Epoch: 119, Loss: 0.001530\n",
      "Epoch: 120, Loss: 0.000968\n",
      "Epoch: 121, Loss: 0.001927\n",
      "Epoch: 122, Loss: 0.001266\n",
      "Epoch: 123, Loss: 0.004289\n",
      "Epoch: 124, Loss: 0.001334\n",
      "Epoch: 125, Loss: 0.000964\n",
      "Epoch: 126, Loss: 0.001065\n",
      "Epoch: 127, Loss: 0.001865\n",
      "Epoch: 128, Loss: 0.000954\n",
      "Epoch: 129, Loss: 0.000784\n",
      "Epoch: 130, Loss: 0.001921\n",
      "Epoch: 131, Loss: 0.000517\n",
      "Epoch: 132, Loss: 0.001074\n",
      "Epoch: 133, Loss: 0.001124\n",
      "Epoch: 134, Loss: 0.000990\n",
      "Epoch: 135, Loss: 0.001107\n",
      "Epoch: 136, Loss: 0.000633\n",
      "Epoch: 137, Loss: 0.000260\n",
      "Epoch: 138, Loss: 0.001159\n",
      "Epoch: 139, Loss: 0.000685\n",
      "Epoch: 140, Loss: 0.000684\n",
      "Epoch: 141, Loss: 0.001053\n",
      "Epoch: 142, Loss: 0.001807\n",
      "Epoch: 143, Loss: 0.001119\n",
      "Epoch: 144, Loss: 0.000760\n",
      "Epoch: 145, Loss: 0.000811\n",
      "Epoch: 146, Loss: 0.000940\n",
      "Epoch: 147, Loss: 0.000901\n",
      "Epoch: 148, Loss: 0.000404\n",
      "Epoch: 149, Loss: 0.000723\n",
      "Epoch: 150, Loss: 0.000683\n",
      "Epoch: 151, Loss: 0.001547\n",
      "Epoch: 152, Loss: 0.000616\n",
      "Epoch: 153, Loss: 0.000760\n",
      "Epoch: 154, Loss: 0.000683\n",
      "Epoch: 155, Loss: 0.000680\n",
      "Epoch: 156, Loss: 0.000533\n",
      "Epoch: 157, Loss: 0.000668\n",
      "Epoch: 158, Loss: 0.001343\n",
      "Epoch: 159, Loss: 0.000794\n",
      "Epoch: 160, Loss: 0.001273\n",
      "Epoch: 161, Loss: 0.000776\n",
      "Epoch: 162, Loss: 0.000355\n",
      "Epoch: 163, Loss: 0.000426\n",
      "Epoch: 164, Loss: 0.001574\n",
      "Epoch: 165, Loss: 0.000549\n",
      "Epoch: 166, Loss: 0.000604\n",
      "Epoch: 167, Loss: 0.000759\n",
      "Epoch: 168, Loss: 0.000628\n",
      "Epoch: 169, Loss: 0.000694\n",
      "Epoch: 170, Loss: 0.001073\n",
      "Epoch: 171, Loss: 0.000586\n",
      "Epoch: 172, Loss: 0.000791\n",
      "Epoch: 173, Loss: 0.000573\n",
      "Epoch: 174, Loss: 0.000615\n",
      "Epoch: 175, Loss: 0.001396\n",
      "Epoch: 176, Loss: 0.000381\n",
      "Epoch: 177, Loss: 0.000899\n",
      "Epoch: 178, Loss: 0.000574\n",
      "Epoch: 179, Loss: 0.000497\n",
      "Epoch: 180, Loss: 0.000662\n",
      "Epoch: 181, Loss: 0.000498\n",
      "Epoch: 182, Loss: 0.000858\n",
      "Epoch: 183, Loss: 0.000964\n",
      "Epoch: 184, Loss: 0.000999\n",
      "Epoch: 185, Loss: 0.000364\n",
      "Epoch: 186, Loss: 0.000654\n",
      "Epoch: 187, Loss: 0.000960\n",
      "Epoch: 188, Loss: 0.000827\n",
      "Epoch: 189, Loss: 0.000350\n",
      "Epoch: 190, Loss: 0.001005\n",
      "Epoch: 191, Loss: 0.000753\n",
      "Epoch: 192, Loss: 0.000384\n",
      "Epoch: 193, Loss: 0.000676\n",
      "Epoch: 194, Loss: 0.000518\n",
      "Epoch: 195, Loss: 0.000567\n",
      "Epoch: 196, Loss: 0.000637\n",
      "Epoch: 197, Loss: 0.000657\n",
      "Epoch: 198, Loss: 0.000285\n",
      "Epoch: 199, Loss: 0.000623\n",
      "1841.6741638183594\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cuda:0')\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "train_loader = torch.utils.data.DataLoader(transformed_cifar10, batch_size=64, shuffle=True, **kwargs)\n",
    "model2 = nn.Sequential(nn.Linear(3072, 512), nn.Tanh(), nn.Linear(512, 256), nn.Tanh(), nn.Linear(256, 128), nn.Tanh(), nn.Linear(128, 10), nn.LogSoftmax(dim=1))\n",
    "model2.to(device)\n",
    "#transformed_cifar10.to(device)\n",
    "loss_fn = nn.NLLLoss()\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD(model2.parameters(), lr=learning_rate)\n",
    "#optimizer.zero_grad()\n",
    "n_epochs = 200\n",
    "start = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model2(imgs.view(batch_size, -1).to(device))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e69cf53",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_model_complexity_info() missing 1 required positional argument: 'input_res'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_40192/3379712191.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmacs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_model_complexity_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mas_strings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_per_layer_stat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{:<30}  {:<8}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Computational complexity: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmacs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{:<30}  {:<8}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Number of parameters: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: get_model_complexity_info() missing 1 required positional argument: 'input_res'"
     ]
    }
   ],
   "source": [
    "macs, params = get_model_complexity_info(model, (3, 32, 32),  as_strings=True, print_per_layer_stat=False, verbose=False)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bead6cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model2.state_dict(), 'model2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd93cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(val_transformed_cifar10, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size=imgs.shape[0]\n",
    "        outputs = model2(imgs.view(batch_size, -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted==labels).sum())\n",
    "    print(\"Accuracy %f\", correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d61916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00eb62c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
